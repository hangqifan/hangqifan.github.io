<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分） | Intouchables</title><meta name="keywords" content="论文解读,NLP常用算法,深度学习"><meta name="author" content="Aqua"><meta name="copyright" content="Aqua"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）作者： Aqua，苑博。 审校：苑博。 翻译：Aqua。 感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！ 3 Model ArchitectureMost competitive neural sequence transduction models ha">
<meta property="og:type" content="article">
<meta property="og:title" content="经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）">
<meta property="og:url" content="http://example.com/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/index.html">
<meta property="og:site_name" content="Intouchables">
<meta property="og:description" content="经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）作者： Aqua，苑博。 审校：苑博。 翻译：Aqua。 感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！ 3 Model ArchitectureMost competitive neural sequence transduction models ha">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png">
<meta property="article:published_time" content="2022-04-09T10:23:22.829Z">
<meta property="article:modified_time" content="2022-04-11T09:31:44.781Z">
<meta property="article:author" content="Aqua">
<meta property="article:tag" content="论文解读">
<meta property="article:tag" content="NLP常用算法">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png"><link rel="shortcut icon" href="/img/touxiang.jpg"><link rel="canonical" href="http://example.com/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-11 17:31:44'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="/css/yejiaojianbian.css"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="Intouchables" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 心情</span></a></li><li><a class="site-page child" href="/myself/"><i class="fa-fw fa fa-id-card"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Intouchables</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 心情</span></a></li><li><a class="site-page child" href="/myself/"><i class="fa-fw fa fa-id-card"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-09T10:23:22.829Z" title="发表于 2022-04-09 18:23:22">2022-04-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-04-11T09:31:44.781Z" title="更新于 2022-04-11 17:31:44">2022-04-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">论文解读</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="经典论文Transformer的解读与总结—-《Attention-is-All-you-need》二（Model-Architecture部分）"><a href="#经典论文Transformer的解读与总结—-《Attention-is-All-you-need》二（Model-Architecture部分）" class="headerlink" title="经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）"></a>经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）</h1><p>作者： Aqua，苑博。</p>
<p>审校：苑博。</p>
<p>翻译：Aqua。</p>
<p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p>
<h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3 Model Architecture"></a>3 Model Architecture</h2><p>Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations <script type="math/tex">(x_1,x_2, \cdots ,x_n)</script> to a sequence of continuous representations <script type="math/tex">z=(z_1,z_2, \cdots ,z_n)</script>. Given z, the decoder then generates an output sequence <script type="math/tex">(y_1,y_2, \cdots ,y_m)</script> of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.</p>
<p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p>
<p>解读：</p>
<p>这一部分其实只是一个对编码-解码器架构（encoder-decoder structure）的简单概述，编码器的主要负责将输入序列<script type="math/tex">(x_1,x_2, \cdots ,x_n)</script>映射到序列<script type="math/tex">z=(z_1,z_2, \cdots ,z_n)</script>。当获得z的时候，解码器一次生成一个元素的符号，如此一步步得到输出序列<script type="math/tex">(y_1,y_2, \cdots ,y_m)</script>。在每一步中，模型都是自回归的，在生成下一个符号时，使用之前生成的符号作为附加的输入。（隐义：这一段其实没啥精彩的地方，属于常规操作一般描述。）</p>
<hr>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409193149.png" alt></p>
<h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p><strong>Encoder:</strong> The encoder is composed of a stack of <em>N</em> = 6 identical layers. Each layer has two sub-layers.  The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.  That is, the output of each sub-layer is <script type="math/tex">LayerNorm(x + Sublayer(x))</script>, where <script type="math/tex">Sublayer(x)</script> is the function implemented by the sub-layer itself.   To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <script type="math/tex">d_{model} = 512</script>. </p>
<p>解读：</p>
<p>这一部分详细介绍了Encoder的组成，Encoder由6层堆叠而成，每一层又包括两个子层分别为Multi-head self-attention子层和Position-wise fully connected feed-forward network子层。每个子层之间又采用残差连接（Res-net）并进行归一化。为方便连接说有子层以及embedding产生的数据长度都为512。</p>
<p>总结：</p>
<p>Encoder有N=6层，每层包括两个sub-layers:</p>
<ol>
<li>第一个sub-layer是multi-head self-attention mechanism，用来计算输入的self-attention</li>
<li>第二个sub-layer是简单的全连接网络。</li>
</ol>
<p>在每个sub-layer我们都模拟了残差网络，每个sub-layer的输出都是</p>
<script type="math/tex; mode=display">
LayerNorm(x + Sublayer(x))</script><p>其中Sublayer(x) 表示Sub-layer对输入x做的映射，为了确保连接，所有的sub-layers和embedding layer输出的维数都相同，维数为 <script type="math/tex">d_{model} = 512</script>。（再形象一点的举个例子，就是一个词输入词，在经过embedding层之后，变为一个长度为512的词向量<script type="math/tex">\textbf{x}=(x_1,x_2, \cdots,x_{512})</script>）</p>
<p>tips: 维数指的是矩阵的列</p>
<hr>
<p><strong>Decoder:</strong> The decoder is also composed of a stack of <em>N</em> = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <em>i</em> can depend only on the known outputs at positions less than <em>i</em>.</p>
<p>解读：</p>
<p>Decoder 也由6层堆叠而成，每一层又分为三个子层Mask multi-head self-attention子层、Multi-head self-attention子层（Encoder-decoder Attention）和Position-wise fully connected feed-forward network子层。每个子层之间依然采用残差连接（Res-net）并进行归一化。</p>
<p>总结：</p>
<p>Decoder也是N=6层，每层包括3个sub-layers：</p>
<ol>
<li>第一个是Masked multi-head self-attention，也是计算输入的self-attention，但是因为是生成过程，因此在时刻 i 的时候，大于 i  的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做Mask。</li>
<li>第二个sub-layer是对encoder的输入进行attention计算。</li>
<li>第三个sub-layer是全连接网络，与Encoder相同。</li>
</ol>
<p>同时Decoder中的self-attention层需要进行修改，因为只能获取到当前时刻之前的输入，因此只对时刻 t 之前的时刻输入进行attention计算，这也称为Mask操作。</p>
<hr>
<h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p>
<p>解读：</p>
<p>注意函数可以描述为将一个query和一组key-value对映射到一个输出Z，其中query、key、value和输出Z都是向量。输出Z为value的加权和，其中分配给每个value的权重由query与相应key的兼容性函数计算。(对Attention进行了一个非常简短的介绍，后文会详细给出计算公式)</p>
<hr>
<h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>We call our particular attention “Scaled Dot-Product Attention” (Figure 2). The input consists of queries and keys of dimension <script type="math/tex">d_k</script> , and values of dimension <script type="math/tex">d_v</script>. We compute the dot products of the query with all keys, divide each by <script type="math/tex">\sqrt{d_k}</script> , and apply a softmax function to obtain the weights on the values. </p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409231056.png" alt></p>
<p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <em>Q</em>.  The keys and values are also packed together into matrices <em>K</em> and <em>V</em> . We compute the matrix of outputs as:</p>
<script type="math/tex; mode=display">
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>The two most commonly used attention functions are additive attention, and dot-product (multi-plicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. </p>
<p>While for small values of  <script type="math/tex">\sqrt{d_k}</script> the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of <script type="math/tex">\sqrt{d_k}</script> . We suspect that for large values of <script type="math/tex">d_k</script>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>.</p>
<p>解读：</p>
<p>在Transformer中使用的Attention是Scaled Dot-Product Attention, 是归一化的点乘Attention，假设输入的query（q）、key（k）维度为 <script type="math/tex">d_k</script> ，value（v）维度为<script type="math/tex">d_v</script> , 那么就计算query和每个key的点乘操作，并除以 <script type="math/tex">\sqrt{d_k}</script>  ，然后应用Softmax函数计算权重。</p>
<script type="math/tex; mode=display">
Attention(q_i,k_i,v_i) = softmax(\frac{q_ik_i^T}{\sqrt{d_k}})v_i</script><p> 在实践中，将query和keys、values分别处理为矩阵Q、K和V。矩阵Q，K和V具体的计算过程如Figure 3所示，其中<script type="math/tex">W^Q</script>，<script type="math/tex">W^K</script>和<script type="math/tex">W^V</script>为Transformer在训练过程中需要更新的权重矩阵。<script type="math/tex">X_1</script>和<script type="math/tex">X_2</script>为经过Embedding层之后输出的词向量。</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410161017.png" alt></p>
<p>有了矩阵Q，K和V之后，计算输出矩阵Z为：</p>
<script type="math/tex; mode=display">
Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中 <script type="math/tex">Q\in R^{m×d_k}</script>、<script type="math/tex">K\in R^{m×d_k}</script>和 <script type="math/tex">V\in R^{m×d_v}</script>，输出矩阵维度为<script type="math/tex">R^{m×d_v}</script> ，这里的m表示输入的词的个数，如果为限定输入词的长度为512，则<script type="math/tex">m=d_{model}</script>，Q，K和V具体形式如Figure 4所示</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410160757.png" alt></p>
<p>对于其中输出Z的计算示意图如Figure 5 所示意，每个词的q会跟每一个k计算得分（相关性或者相似度），经过Softmax后就得到整个加权结果，此时每个词看的不只是它前面的序列，而是整个输入序列，通过<script type="math/tex">Attention(Q,K,V)</script>之后同一时间便可以计算出所有词的表示结果。</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410163644.png" alt></p>
<p>Scaled Dot-Product Attention的整体示意图如Figure 2左图所示，Mask是可选的(opt.)，如果是能够获取到所有时刻的输入(K, V), 那么就不使用Mask；如果是不能获取到，那么就需要使用Mask。使用了Mask的Transformer模型也被称为Transformer Decoder，不使用Mask的Transformer模型也被称为Transformer Encoder。</p>
<p>为什么要除以<script type="math/tex">\sqrt{d_k}</script> ?，作者这样说到：</p>
<p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, <script type="math/tex">qk = \sum_{i=1}^{d_k}q_ik_i</script>, has mean 0 and variance <script type="math/tex">\sqrt{d_k}</script>.</p>
<p>因此，每个分量除以<script type="math/tex">\sqrt{d_k}</script>可以让点乘的方差变为1。</p>
<hr>
<h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>Instead of performing a single attention function with <script type="math/tex">d_{model}</script>-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values <em>h</em> times with different, learned linear projections to <script type="math/tex">d_k</script>, <em><script type="math/tex">d_k</script></em> and <script type="math/tex">d_v</script> dimensions, respectively.  On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding <script type="math/tex">d_{v}</script>​-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. </p>
<p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O</script><script type="math/tex; mode=display">
where \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>Where the projections are parameter matrices <script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script> and  <script type="math/tex">W^O \in R^{hd_{v}×d_{model}}</script>.In this work we employ <em>h</em> = 8 parallel attention layers, or heads. For each of these we use <script type="math/tex">d_k = d_v = d_{model}/h=64</script>. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. </p>
<p>解读：</p>
<p>学习深度网络的过程，如果能够把每一层输入输出的张量大小搞懂了，那么这个网络基本上搞懂了。尤其在学习和调试Transformer代码过程中，如果能够把每一步的输入输出搞清楚，那么源码的每一步在做什么事情，也就基本能和论文对应起来了。所以说到这一步我们必须把文章前面的几个小细节回顾一下，然后才能搞懂这一部分的公式在讲什么。</p>
<ul>
<li><strong>Part one</strong>  不管是Encoder还是Decoder部分，所有的sub-layers和embedding layer输出的维数都相同，维数为 <script type="math/tex">d_{model} = 512</script>。所以<script type="math/tex">X \in R^{m×d_{model}}</script>， <script type="math/tex">Z \in R^{m×d_{model}}</script>，其中m表示输入x的个数。</li>
<li><strong>Part two</strong>   在3.2.1中Q、K和V大小<script type="math/tex">Q\in R^{m×d_k}</script>、<script type="math/tex">K\in R^{m×d_k}</script>和 <script type="math/tex">V\in R^{m×d_v}</script>，因为<script type="math/tex">Q=XW^Q</script>, 并且已知X和Q的大小，则<script type="math/tex">W^Q \in R^{d_{model}×d_k}</script>，同理<script type="math/tex">W^K \in R^{d_{model}×d_k}</script>，<script type="math/tex">W^V \in R^{d_{model}×d_v}</script>, 输出Z的大小通过<script type="math/tex">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script>，可得<script type="math/tex">Z\in R^{m×d_v}</script>。</li>
</ul>
<p>接下来分析<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>中的Q，K和V的大小。</p>
<ol>
<li><strong>Part one</strong>中要求<script type="math/tex">Z \in R^{m×d_{model}}</script>，2中推得<script type="math/tex">Z\in R^{m×d_v}</script>，他们都指同一个Z，结合1和2这两条，可得在3.2.1 Scaled Dot-Product Attention中得到$d<em>v$的大小必须和$$d</em>{model}<script type="math/tex">保持一致都为512，即</script>d<em>v = d</em>{model}$$。</li>
<li>通过<strong>Part two</strong> 还可以看到，而其中的<script type="math/tex">d_k</script>并不影响Z的维数，所以<script type="math/tex">d_k</script>的大小感觉是没有要求的。但是在论文中本部分中要求<script type="math/tex">d_k = d_v = d_{model}/h=64</script>。</li>
<li>对于<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>中的Q，K和V的大小而言，因为<script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script>。要想使得式中的Q，K，V满足和对应的<script type="math/tex">W_i^Q,W_i^K,W_i^V</script>相乘，则<script type="math/tex">Q\in R^{m×d_{model}}</script>、<script type="math/tex">K\in R^{m×d_{model}}</script>和 <script type="math/tex">V\in R^{m×d_{model}}</script>。</li>
</ol>
<p>对于<script type="math/tex">MultiHead(Q,K,V)</script>的公式解读：</p>
<ul>
<li>通过以上分析知道<script type="math/tex">Q\in R^{m×d_{model}}</script>，<script type="math/tex">K\in R^{m×d_{model}}</script>，<script type="math/tex">V\in R^{m×d_{model}}</script>和<script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script>，通过矩阵乘法法则可得<script type="math/tex">QW_i^Q\in R^{m×d_k}</script>、<script type="math/tex">KW_i^K\in R^{m×d_k}</script>和 <script type="math/tex">VW_i^V\in R^{m×d_v}</script>，因为<script type="math/tex">d_k = d_{model}/h=64</script>，相当于将原来<script type="math/tex">Q\in R^{m×d_{model}}</script>、<script type="math/tex">K\in R^{m×d_{model}}</script>和 <script type="math/tex">V\in R^{m×d_{model}}</script>的维数由<script type="math/tex">d_{model}=512</script>映射到了更低的维度<script type="math/tex">d_{model}/h=64</script>。</li>
<li>然后在采用Scaled Dot-Product Attention计算出每一个head结果，即<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>。因为在<strong>part two</strong>中，输出Z即为<script type="math/tex">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script>的计算结果，并且其中<script type="math/tex">Z\in R^{m×d_v}</script>和V的大小保持一致。以此类推<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>，因为<script type="math/tex">VW_i^V\in R^{m×d_v}</script> ，又因为<script type="math/tex">d_v = d_{model}/h=64</script>，所以<script type="math/tex">head_i \in R^{m×d_v}=R^{m×d_{model}/h}</script>。</li>
<li>将总共h个head的结果进行按列拼接起来，一个head是<script type="math/tex">d_v=d_{model}/h=64</script>列，h个head拼接完总共有<script type="math/tex">hd_{v}=d_{model}=512</script>列， 即<script type="math/tex">Concat(head_1,\cdots,head_h) \in R^{m × hd_v}</script>。</li>
<li>将合并的结果进行线性变换，因为<script type="math/tex">Concat(head_1,\cdots,head_h) \in R^{m × hd_v}</script>和，<script type="math/tex">W^O \in R^{hd_{v}×d_{model}}</script>根据矩阵乘法法则，<script type="math/tex">MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O \in R^{m×d_{model}}</script>。</li>
</ul>
<p>其更细致的流程图如Figure 6所示</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410231003.png" alt></p>
<hr>
<h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>The Transformer uses multi-head attention in three different ways:</p>
<ul>
<li>In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as.</li>
<li>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li>
<li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to <script type="math/tex">-\infty</script>) all values in the input of the softmax which correspond to illegal connections. See Figure 2.</li>
</ul>
<p>解读：</p>
<ul>
<li>Encoder模块的Self-Attention，在Encoder中，每层的Self-Attention的输入<script type="math/tex">Q=K=V=X</script> ，都是上一层的输出。Encoder中的每个position都能够获取到前一层的所有位置的输出。</li>
<li>Decoder模块的Mask Self-Attention，在Decoder中，每个position只能获取到之前position的信息，因此需要做mask，将其设置为<script type="math/tex">-\infty</script>。</li>
<li>Encoder-Decoder之间的Attention，其中Q来自于之前的Decoder层输出，K和V来自于encoder的输出，这样decoder的每个位置都能够获取到输入序列的所有位置信息。</li>
</ul>
<hr>
<h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a>3.3 Position-wise Feed-Forward Networks</h3><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</p>
<script type="math/tex; mode=display">
FFN(x)= max(0,xW_1+b_1)W_2+b_2</script><p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is <script type="math/tex">d_{model}=512</script>, and the inner-layer has dimensionality <script type="math/tex">d_{ff} = 2048</script>.</p>
<p>解读：</p>
<p>在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，这个层与一般的全连接不一样，对每个position的向量分别进行相同的操作，即使用相同的参数分别进行计算。总共包括两个线性变换和一个ReLU激活输出，看起来更像是两个w=1的卷积，将输入<script type="math/tex">d_{model}=512</script>的矩阵转为<script type="math/tex">d_{ff} = 2048</script>，之后再转为原始大小。除此之外其中每一层的参数都不同。</p>
<hr>
<h3 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4 Embeddings and Softmax"></a>3.4 Embeddings and Softmax</h3><p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension <script type="math/tex">d_{model}</script>. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by <script type="math/tex">\sqrt{d_{model}}</script>.</p>
<p>解读：</p>
<p>同其它模型一样，transformer的word emebdding层将输入token转换为<script type="math/tex">d_{model}</script>维词向量。在解码时，同样对输出先进行线性变换，再进行softmax归一化，得到下一个词的概率分布。</p>
<p>Embedding：</p>
<p>Transformer中使用了三处embedding：1）input embedding；2）output embedding；3）在softmax前面的Linear transformation中也使用跟embedding相同的权重。三处的embedding权重参数是共享的，这样做可以有效减少模型参数量。我们知道embedding层和Linear层的参数大小是vocab size × embedding size，其中embedding size=<script type="math/tex">d_{model}</script>，而且通常vocab size比embedding size高两个数量级，因此，共享参数能显著减少模型参数量。另外值得注意的点就是，作者把embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>，学 embedding 的时候，会把每一个向量的 Lar Norm 学的比较小。维度大的话，学到的一些权重值就会变小，但之后还需要加上 positional encoding（不会随着维度的增加而变化）。embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>使得 embedding中的权重参数 和 positional encosing中的权重参数的 scale 差不多，可以做加法。</p>
<p>Liner层和Softmax层：</p>
<p>解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。其详细输入输出的流程图如Figure 7。</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411122219.png" alt></p>
<hr>
<h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h3><p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the. bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <script type="math/tex">d_{model}</script> as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed. In this work, we use sine and cosine functions of different frequencies:</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/dmodel})</script><script type="math/tex; mode=display">
PE_{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})</script><p>where <em>pos</em> is the position and <em>i</em> is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2<em>π</em> to 10000 <em>·</em> 2<em>π</em>. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <em>k</em>, <script type="math/tex">PE_{pos+k}</script> can be represented as a linear function of <script type="math/tex">PE_{pos}</script>.</p>
<p>We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p>
<p>解读：</p>
<p>一个传统的，使用 RNN 构建的 Encoder-Decoder 模型对于输入句子中的单词是逐个读取的：读取完前一个单词，更新模型的状态，然后在此状态下再读取下一个单词。这种方式天然的包含了句子的位置前后关系。CNN模型可以有卷积的多次采样扩展得到位置信息。</p>
<p>Transformer模型不包括recurrence/convolution，其对整个句子的每个单词进行同时读取，因此是无法捕捉到序列顺序信息的。假如将词序打乱，或者说将K、V按行进行打乱，那么Attention之后的结果是一样的。这就与人类的语言常识相悖，通常来看，在处理时序数据的时候，如果将一句话里面的词打乱或颠倒顺序，那么语义肯定会发生变化，但是 attention 不会处理这个情况。 因此为了能够保留句子中单词和单词之间的位置关系，需要加入时序信息。也就是需要将位置也融合进入输入的句子中，即对位置进行编码。</p>
<p>Transformer 使用的位置编码计算公式如下：</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/dmodel})</script><script type="math/tex; mode=display">
PE_{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})</script><p>其中<em>pos</em>表示位置index， <em>i</em>表示dimension index。由公式来看，对于一个句子，此编码算法对于偶数位置 <em>2i</em> 和奇数位置 <em>2i+1</em> 分开进行编码。编码的结果是每个位置最终转化为<script type="math/tex">d_{model}</script>维度的向量。</p>
<p>虽然现在知道了Transformer的位置编码方式，学习讲究知其然知其所以然，接下来再简析一下Transformer位置编码的特点和原理。将位置编码公式拆解微观分析得：</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411151747.png" alt></p>
<p>并将位置编码向量按照热力图的形式进行展示得：</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411152231.png" alt></p>
<p>上述横坐标代表维度<em>i</em>，纵坐标代表位置。可以看到，对于句子长度最大为50的模型来说，前60维就可以区分位置了。至于后面为什么一白一蓝，别忘了，sin,cos相间。</p>
<p>Position Embedding本身是一个绝对位置的信息，仔细观察Transformer论文给出位置编码的公式可以发现，即只要位置小于10000，每一个位置的编码都是不同的。但在语言中，相对位置也很重要，绝对位置信息只保证了各个位置不一样，但是并不是像0,1,2这样的有明确前后关系的编码。所以需要相对位置来保证语序对应的语义。由于三角函数两角和差公式，Transformer论文给出位置编码的公式也可以很好的表示相对位置信息。</p>
<script type="math/tex; mode=display">
sin(\alpha+\beta) = sin(\alpha)cos(\beta) + cos(\alpha)sin(\beta)</script><script type="math/tex; mode=display">
cos(\alpha+\beta) = cos(\alpha)cos(\beta) - sin(\alpha)cos(\beta)</script><p>给定k,存在一个固定的与k相关的线性变换矩阵，从而由<em>pos</em>的位置编码线性变换而得到<em>pos+k</em>的位置编码。从而这个相对位置信息也可以被模型发现并利用。其证明如下：</p>
<p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411153722.png" alt></p>
<p>序列信息代表着全局的结构，因此必须将序列的token相对或者绝对position信息利用起来。这里每个token的position embedding 向量维度也是<script type="math/tex">d_{model}=512</script>，然后将原本的embedding和position embedding加起来组成最终的embedding作为encoder/decoder的输入。positional embedding是 cos 和 sin 的一个函数，在 [-1, +1] 之间抖动的。所以embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>，使得乘积后的每个权重也是在差不多的 [-1, +1] 数值区间。相加完成在输入里面添加时序信息。不管怎么打乱输入序列的顺序，进入 layer 之后，输出那些值是不变的，最多是顺序发生了相应的变化。所以就直接把顺序信息直接加在数据值里。</p>
<p>:sailboat:positional embedding被加到embedding中。嵌入表示一个 <script type="math/tex">d_{model}</script>维空间的标记，在<script type="math/tex">d_{model}</script>维空间中有着相似含义的标记会离彼此更近。但是，嵌入并没有对在一句话中的词的相对位置进行编码。因此，当加上位置编码后，词将基于它们含义的相似度以及它们在句子中的位置，在<script type="math/tex">d_{model}</script> 维空间中离彼此更近。</p>
<p>在其他NLP论文中，大家也都看过position embedding，通常是一个训练的向量，但是position embedding只是extra features，有该信息会更好，但是没有性能也不会产生极大下降，因为RNN、CNN本身就能够捕捉到位置信息，但是在Transformer模型中，Position Embedding是位置信息的唯一来源，因此是该模型的核心成分，并非是辅助性质的特征。</p>
<p>也可以采用训练的position embedding，但是试验结果表明相差不大，因此论文选择了sin position embedding，因为</p>
<ol>
<li>这样可以直接计算embedding而不需要训练，减少了训练参数</li>
<li>这样允许模型将position embedding扩展到超过了training set中最长position的position，例如测试集中出现了更大的position，sin position embedding依然可以给出结果，但不存在训练到的embedding。</li>
</ol>
<hr>
<h2 id="参考及感谢"><a href="#参考及感谢" class="headerlink" title="参考及感谢"></a>参考及感谢</h2><p>Transformer Encoder部分：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/343286144">https://zhuanlan.zhihu.com/p/343286144</a></p>
<p>李沐讲解Transformer：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/452663865">https://zhuanlan.zhihu.com/p/452663865</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/460607065">https://zhuanlan.zhihu.com/p/460607065</a></p>
<p>一个很粗糙的代码解读：</p>
<p><a target="_blank" rel="noopener" href="https://baijiahao.baidu.com/s?id=1711290350168224964&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1711290350168224964&amp;wfr=spider&amp;for=pc</a></p>
<p>Transformer理解，看这篇文章读懂了embedding为什么参数共享</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/bingmeishi/article/details/105105823">https://blog.csdn.net/bingmeishi/article/details/105105823</a></p>
<p>Transform训练过程：</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">https://blog.csdn.net/longxinchen_ml/article/details/86533005</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></p>
<p>【从 0 开始学习 Transformer】 上篇：Transformer 搭建与理解：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97448796">https://zhuanlan.zhihu.com/p/97448796</a></p>
<p>【从 0 开始学习 Transformer】 下篇：Transformer 训练与评估：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></p>
<p>自注意动图展示：</p>
<p><a target="_blank" rel="noopener" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p>
<p>深度学习中的注意力模型（2017版）：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/37601161">https://zhuanlan.zhihu.com/p/37601161</a></p>
<p>Transformer学习笔记一：Positional Encoding（位置编码）</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/454482273">https://zhuanlan.zhihu.com/p/454482273</a></p>
<p>Transformer架构：位置编码</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/Jayson13/article/details/123135888">https://blog.csdn.net/Jayson13/article/details/123135888</a></p>
<p>深入理解transformer中的位置编码</p>
<p><a target="_blank" rel="noopener" href="https://www.csdn.net/tags/OtDacg3sNDA1NjUtYmxvZwO0O0OO0O0O.html">https://www.csdn.net/tags/OtDacg3sNDA1NjUtYmxvZwO0O0OO0O0O.html</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_43391414/article/details/121061766">https://blog.csdn.net/qq_43391414/article/details/121061766</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Aqua</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/">http://example.com/2022/04/09/经典论文中的解读与厚黑---《Attention is All you need》（Model Architecture部分）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Intouchables</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">论文解读</a><a class="post-meta__tags" href="/tags/NLP%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/">NLP常用算法</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/weichat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/weichat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/zhifubao.jpg" target="_blank"><img class="post-qr-code-img" src="/img/zhifubao.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/"><img class="prev-cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/"><img class="next-cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/" title="经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）"><img class="cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-08</div><div class="title">经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）</div></div></a></div><div><a href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/" title="经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）"><img class="cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-11</div><div class="title">经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Aqua</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">18</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">10</div></a></div></div><a id="card-info-btn" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1329586354&amp;website=www.oicqzone.com"><i class="iconfont icon-QQ"></i><span>联系博主</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/hangqifan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1329586354@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://gitee.com/fanhangqi" target="_blank" title="Gitee"><i class="iconfont icon-gitee2 card_icon"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">有一份NLP小知识需要您查收</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93%E2%80%94-%E3%80%8AAttention-is-All-you-need%E3%80%8B%E4%BA%8C%EF%BC%88Model-Architecture%E9%83%A8%E5%88%86%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Model-Architecture"><span class="toc-number">1.1.</span> <span class="toc-text">3 Model Architecture</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Encoder-and-Decoder-Stacks"><span class="toc-number">1.1.1.</span> <span class="toc-text">3.1 Encoder and Decoder Stacks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Attention"><span class="toc-number">1.1.2.</span> <span class="toc-text">3.2 Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Scaled-Dot-Product-Attention"><span class="toc-number">1.1.2.1.</span> <span class="toc-text">3.2.1 Scaled Dot-Product Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Multi-Head-Attention"><span class="toc-number">1.1.2.2.</span> <span class="toc-text">3.2.2 Multi-Head Attention</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-Applications-of-Attention-in-our-Model"><span class="toc-number">1.1.2.3.</span> <span class="toc-text">3.2.3 Applications of Attention in our Model</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Position-wise-Feed-Forward-Networks"><span class="toc-number">1.1.3.</span> <span class="toc-text">3.3 Position-wise Feed-Forward Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Embeddings-and-Softmax"><span class="toc-number">1.1.4.</span> <span class="toc-text">3.4 Embeddings and Softmax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-Positional-Encoding"><span class="toc-number">1.1.5.</span> <span class="toc-text">3.5 Positional Encoding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E5%8F%8A%E6%84%9F%E8%B0%A2"><span class="toc-number">1.2.</span> <span class="toc-text">参考及感谢</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E6%96%87%E4%BB%B6/" title="Python六式---文件"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---文件"/></a><div class="content"><a class="title" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E6%96%87%E4%BB%B6/" title="Python六式---文件">Python六式---文件</a><time datetime="2022-04-29T12:38:37.947Z" title="发表于 2022-04-29 20:38:37">2022-04-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E6%A8%A1%E5%9D%97%E5%92%8C%E5%8C%85/" title="Python六式---模块和包"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---模块和包"/></a><div class="content"><a class="title" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E6%A8%A1%E5%9D%97%E5%92%8C%E5%8C%85/" title="Python六式---模块和包">Python六式---模块和包</a><time datetime="2022-04-29T10:02:34.473Z" title="发表于 2022-04-29 18:02:34">2022-04-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/29/Python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F---%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" title="Python设计模式---单例模式"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220429170903.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python设计模式---单例模式"/></a><div class="content"><a class="title" href="/2022/04/29/Python%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F---%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/" title="Python设计模式---单例模式">Python设计模式---单例模式</a><time datetime="2022-04-29T08:50:15.873Z" title="发表于 2022-04-29 16:50:15">2022-04-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E7%B1%BB%E6%96%B9%E6%B3%95/" title="Python六式---类属性和类方法"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---类属性和类方法"/></a><div class="content"><a class="title" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E7%B1%BB%E5%B1%9E%E6%80%A7%E5%92%8C%E7%B1%BB%E6%96%B9%E6%B3%95/" title="Python六式---类属性和类方法">Python六式---类属性和类方法</a><time datetime="2022-04-29T08:18:41.993Z" title="发表于 2022-04-29 16:18:41">2022-04-29</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E7%BB%A7%E6%89%BF/" title="Python六式---继承和多态"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---继承和多态"/></a><div class="content"><a class="title" href="/2022/04/29/Python%E5%85%AD%E5%BC%8F---%E7%BB%A7%E6%89%BF/" title="Python六式---继承和多态">Python六式---继承和多态</a><time datetime="2022-04-29T07:05:20.663Z" title="发表于 2022-04-29 15:05:20">2022-04-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Aqua</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">愿天下没有难做的程序员</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'qCtkaFo7Cj3pWewJ6hkHBM5H-gzGzoHsz',
      appKey: 'iV8EVGuBIcJUDcNzQwnzc2t3',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script type="text/javascript" src="/js/fairyDustCursor.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>