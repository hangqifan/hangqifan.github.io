<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分） | Intouchables</title><meta name="keywords" content="论文解读,NLP常用算法,深度学习"><meta name="author" content="Aqua"><meta name="copyright" content="Aqua"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）作者： Aqua，苑博。 审校：苑博。 翻译：Aqua。 感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！ 《Attention is All you need》你真的读懂了吗？不管读没读过，这篇论文都值得多读几遍。 1 IntroductionRecurrent ne">
<meta property="og:type" content="article">
<meta property="og:title" content="经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）">
<meta property="og:url" content="http://example.com/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/index.html">
<meta property="og:site_name" content="Intouchables">
<meta property="og:description" content="经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）作者： Aqua，苑博。 审校：苑博。 翻译：Aqua。 感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！ 《Attention is All you need》你真的读懂了吗？不管读没读过，这篇论文都值得多读几遍。 1 IntroductionRecurrent ne">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png">
<meta property="article:published_time" content="2022-04-08T08:48:33.992Z">
<meta property="article:modified_time" content="2022-04-11T08:56:23.499Z">
<meta property="article:author" content="Aqua">
<meta property="article:tag" content="论文解读">
<meta property="article:tag" content="NLP常用算法">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png"><link rel="shortcut icon" href="/img/touxiang.jpg"><link rel="canonical" href="http://example.com/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-04-11 16:56:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/mycss.css"><link rel="stylesheet" href="/css/yejiaojianbian.css"><meta name="generator" content="Hexo 6.1.0"><link rel="alternate" href="/atom.xml" title="Intouchables" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/touxiang.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 心情</span></a></li><li><a class="site-page child" href="/myself/"><i class="fa-fw fa fa-id-card"></i><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Intouchables</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/messageboard/"><i class="fa-fw fa fa-paper-plane"></i><span> 留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fa fa-heartbeat"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/timeline/"><i class="fa-fw fa fa-bell"></i><span> 日志</span></a></li><li><a class="site-page child" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 心情</span></a></li><li><a class="site-page child" href="/myself/"><i class="fa-fw fa fa-id-card"></i><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-08T08:48:33.992Z" title="发表于 2022-04-08 16:48:33">2022-04-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-04-11T08:56:23.499Z" title="更新于 2022-04-11 16:56:23">2022-04-11</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">论文解读</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/NLP/">NLP</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="经典论文的解读与厚黑—-《Attention-is-All-you-need》一（Introduction和Background部分）"><a href="#经典论文的解读与厚黑—-《Attention-is-All-you-need》一（Introduction和Background部分）" class="headerlink" title="经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）"></a>经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）</h1><p>作者： Aqua，苑博。</p>
<p>审校：苑博。</p>
<p>翻译：Aqua。</p>
<p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p>
<p>《Attention is All you need》你真的读懂了吗？不管读没读过，这篇论文都值得多读几遍。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Recurrent neural networks, long short-term memory and gated recurrent  neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.</p>
<p>解读与厚黑：</p>
<p>目前来看，循环神经网络（RNN），如长短期记忆和门控神经网络模型（LSTM），在语言模型和机器翻译领域一直处于主导地位。可惜的是现在的大部分研究和努力只是游离在RNN和编码器-解码器架构的边界而已，意义不大。（隐义：现在循环神经网络很火，虽然大家都在凑这个热闹，但都是些小打小闹，没啥突破性进展。是时候换换天了。为Transformer做铺垫）</p>
<hr>
<p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <script type="math/tex">h_t</script> , as a function of the previous hidden state <script type="math/tex">h_{t-1}</script>​​​ and the input for position <em>t</em>. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p>
<p>解读与厚黑：</p>
<p>简单说说RNN存在的问题， t时刻的计算<script type="math/tex">h_t</script>依赖于 t-1 时刻的计算结果<script type="math/tex">h_{t-1}</script>，这种由前往后顺序计算的形式，会存在长距离的依赖问题，这样不仅限制了模型的并行能力。还使得当前词语获取到的上下文信息受限。即使现在的很多研究想通过factorization tricks 和conditional computation来提升模型的计算效率和性能表现，但是依然没有改变RNN天生顺序约束的缺陷。（隐义：RNN的这种顺序的结构设计就有问题，你们这些跟随者即使做再多RNN局部优化的尝试，也多是隔靴挠痒，意义不大。只有从根本上改变，才能实现真正的突破，比如注意力机制）</p>
<hr>
<p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.</p>
<p>解读与厚黑：</p>
<p>众所周知，在翻译过程中，不同的英文对中文的依赖程度不同。还有文本在表征语义时，当前词需要获取的信息可能来自距离很远的另一个词。 注意力机制（Attention mechanisms）通过将序列中任意两个位置之间的距离缩小为一个常量 ，从而解决长距离依赖的问题。这种对依赖关系进行建模的方式，不仅可以很好的适应了我们人脑的认知，还能够实现并行。在大多数情况下，注意机制与循环网络一起使用往往会产生很好的效果。（隐义：你看注意力机制就不一样了吧，把你最根本的问题解决了，并且我还发现Attention搭配上RNN，效果倍棒。方向给你们了，自己看着办！）</p>
<hr>
<p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p>
<p>解读与厚黑：</p>
<p>Transformer的核心就是使用Attention来绘制输入和输出之间的全局依赖关系，除此之外，还可以使用并行训练的方式大大缩短训练时间。（隐义：咱们这个Transformer内部用的可是Attention，训练快效果好，洋气又时髦，了解一下啊大哥）</p>
<hr>
<h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet  and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.  In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p>
<p>解读与厚黑：</p>
<p>RNN的最大问题在于顺序计算，虽然通过卷积神经网络的方式可以减少顺序计算，实现并行计算。但是模型在学习任意两个位置之间的依赖关系时，所需的操作数随着距离的增长而增长。这就使得模型学习遥远位置之间的依赖关系要更加困难。而在Transformer中，学习任意两个位置之间的依赖关系所需的操作数被减少为恒定次数。所以Transformer要相比于卷积的形式就来的更加有效率。（隐义：虽然你卷积可以并行，但是你有长距离难计算的问题啊，我的Transformer就不同了，不仅具有你并行的好处，还能轻松计算任意位置间的依赖关系。卷积你不太行啊！）</p>
<hr>
<p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.</p>
<p>解读与厚黑：</p>
<p>自注意机制作为注意机制中的一种，将单个序列的不同位置联系起来，以计算序列的表示。同时自注意机制已成功地应用于各种任务中，包括阅读理解、抽象摘要和文本隐含等。（隐义：自注意力机制对各种NLP任务的适用性都挺好，东西不错，Transformer也用用。）</p>
<hr>
<p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks</p>
<p>直接厚黑：</p>
<p>End-to-end memory networks也是个好东西，借鉴一下。</p>
<hr>
<p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models.</p>
<p>解读与厚黑：</p>
<p>Transformer 是第一个完全依赖于Self-attention来计算其输入和输出表示的transduction model，而不使用序列对齐的RNN或卷积神经网络。（隐义：Transformer结合了Self-attention和End-to-end memory networks的好处，可以说是开创性尝试，下面正式开吹。）</p>
<hr>
<h2 id="参考及感谢"><a href="#参考及感谢" class="headerlink" title="参考及感谢"></a>参考及感谢</h2><p>LSTM 已死，事实真是这样吗？</p>
<p><a target="_blank" rel="noopener" href="https://view.inews.qq.com/a/20220325A03LNX00">https://view.inews.qq.com/a/20220325A03LNX00</a></p>
<p>Attention is all you need 详解Transformer</p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/zhanghaiyan/p/11079504.html">https://www.cnblogs.com/zhanghaiyan/p/11079504.html</a></p>
<p>Attention is all you need博客</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_33741547/article/details/85884052">https://blog.csdn.net/sinat_33741547/article/details/85884052</a></p>
<p>论文解读:Attention is All you need</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46990010">https://zhuanlan.zhihu.com/p/46990010</a></p>
<p>长依赖问题：</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/69704935">https://zhuanlan.zhihu.com/p/69704935</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Aqua</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/">http://example.com/2022/04/08/经典论文中的解读与厚黑---《Attention is All you need》（Introduction和Background部分）/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Intouchables</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB/">论文解读</a><a class="post-meta__tags" href="/tags/NLP%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/">NLP常用算法</a><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></div><div class="post_share"><div class="social-share" data-image="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i> 打赏</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/weichat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/weichat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/zhifubao.jpg" target="_blank"><img class="post-qr-code-img" src="/img/zhifubao.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/"><img class="prev-cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）</div></div></a></div><div class="next-post pull-right"><a href="/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"><img class="next-cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406215559.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">面试算法100问---两数相加</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/" title="经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）"><img class="cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-11</div><div class="title">经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）</div></div></a></div><div><a href="/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/" title="经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）"><img class="cover" src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-04-09</div><div class="title">经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/touxiang.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Aqua</div><div class="author-info__description"></div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">文章</div><div class="length-num">10</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div></div><a id="card-info-btn" href="tencent://AddContact/?fromId=45&amp;fromSubId=1&amp;subcmd=all&amp;uin=1329586354&amp;website=www.oicqzone.com"><i class="iconfont icon-QQ"></i><span>联系博主</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/hangqifan" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:1329586354@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://gitee.com/fanhangqi" target="_blank" title="Gitee"><i class="iconfont icon-gitee2 card_icon"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">有一份NLP小知识需要您查收</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91%E2%80%94-%E3%80%8AAttention-is-All-you-need%E3%80%8B%E4%B8%80%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-number">1.1.</span> <span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Background"><span class="toc-number">1.2.</span> <span class="toc-text">2 Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E5%8F%8A%E6%84%9F%E8%B0%A2"><span class="toc-number">1.3.</span> <span class="toc-text">参考及感谢</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%88%A4%E6%96%AD%E5%92%8C%E5%BE%AA%E7%8E%AF/" title="Python六式---判断和循环"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---判断和循环"/></a><div class="content"><a class="title" href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%88%A4%E6%96%AD%E5%92%8C%E5%BE%AA%E7%8E%AF/" title="Python六式---判断和循环">Python六式---判断和循环</a><time datetime="2022-04-22T07:48:47.086Z" title="发表于 2022-04-22 15:48:47">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%8F%98%E9%87%8F%E7%9A%84%E5%91%BD%E5%90%8D%E5%92%8C%E8%BF%90%E7%AE%97%E7%AC%A6/" title="Python六式---变量的命名和运算符"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---变量的命名和运算符"/></a><div class="content"><a class="title" href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%8F%98%E9%87%8F%E7%9A%84%E5%91%BD%E5%90%8D%E5%92%8C%E8%BF%90%E7%AE%97%E7%AC%A6/" title="Python六式---变量的命名和运算符">Python六式---变量的命名和运算符</a><time datetime="2022-04-22T06:48:29.763Z" title="发表于 2022-04-22 14:48:29">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6%E5%92%8C%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/" title="Python六式---算数运算符和变量的基本使用"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422142027.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python六式---算数运算符和变量的基本使用"/></a><div class="content"><a class="title" href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6%E5%92%8C%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/" title="Python六式---算数运算符和变量的基本使用">Python六式---算数运算符和变量的基本使用</a><time datetime="2022-04-22T05:30:20.793Z" title="发表于 2022-04-22 13:30:20">2022-04-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/18/Python%E8%AF%BB%E5%86%99excel%E8%84%9A%E6%9C%AC---%E6%A0%B9%E6%8D%AE%E5%9C%B0%E5%9D%80%E6%8F%90%E5%8F%96%E7%9C%81%E5%B8%82%E5%8E%BF%E5%8C%BA/" title="Python读写excel脚本---根据地址提取省市县区"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220418141024.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Python读写excel脚本---根据地址提取省市县区"/></a><div class="content"><a class="title" href="/2022/04/18/Python%E8%AF%BB%E5%86%99excel%E8%84%9A%E6%9C%AC---%E6%A0%B9%E6%8D%AE%E5%9C%B0%E5%9D%80%E6%8F%90%E5%8F%96%E7%9C%81%E5%B8%82%E5%8E%BF%E5%8C%BA/" title="Python读写excel脚本---根据地址提取省市县区">Python读写excel脚本---根据地址提取省市县区</a><time datetime="2022-04-18T05:55:58.557Z" title="发表于 2022-04-18 13:55:58">2022-04-18</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/" title="经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）"><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409173612.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）"/></a><div class="content"><a class="title" href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/" title="经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）">经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）</a><time datetime="2022-04-11T08:55:16.839Z" title="发表于 2022-04-11 16:55:16">2022-04-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 By Aqua</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">愿天下没有难做的程序员</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">本地搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'qCtkaFo7Cj3pWewJ6hkHBM5H-gzGzoHsz',
      appKey: 'iV8EVGuBIcJUDcNzQwnzc2t3',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script type="text/javascript" src="/js/fairyDustCursor.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>