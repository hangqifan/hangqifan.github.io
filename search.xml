<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>经典论文中的解读与厚黑---《Attention is All you need》（Introduction和Background部分）</title>
      <link href="/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文中的解读与厚黑—-《Attention-is-All-you-need》（Introduction和Background部分）"><a href="#经典论文中的解读与厚黑—-《Attention-is-All-you-need》（Introduction和Background部分）" class="headerlink" title="经典论文中的解读与厚黑—-《Attention is All you need》（Introduction和Background部分）"></a>经典论文中的解读与厚黑—-《Attention is All you need》（Introduction和Background部分）</h1><p>《Attention is All you need》你真的读懂了吗？不管读没读过，这篇论文都值得多读几遍。</p><script type="math/tex; mode=display">H_i</script><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Recurrent neural networks, long short-term memory and gated recurrent  neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.</p><p>解读与厚黑：</p><p>目前来看，循环神经网络（RNN），如长短期记忆和门控神经网络模型（LSTM），在语言模型和机器翻译领域一直处于主导地位。可惜的是现在的大部分研究和努力只是游离在RNN和编码器-解码器架构的边界而已，意义不大。（隐义：现在循环神经网络很火，虽然大家都在凑这个热闹，但都是些小打小闹，没啥突破性进展。是时候换换天了。为Transformer做铺垫）</p><hr><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <script type="math/tex">h_t</script> , as a function of the previous hidden state <script type="math/tex">h_{t-1}</script>​​​ and the input for position <em>t</em>. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p><p>解读与厚黑：</p><p>简单说说RNN存在的问题， t时刻的计算<script type="math/tex">h_t</script>依赖于 t-1 时刻的计算结果<script type="math/tex">h_{t-1}</script>，这种由前往后顺序计算的形式，会存在长距离的依赖问题，这样不仅限制了模型的并行能力。还使得当前词语获取到的上下文信息受限。即使现在的很多研究想通过factorization tricks 和conditional computation来提升模型的计算效率和性能表现，但是依然没有改变RNN天生顺序约束的缺陷。（隐义：RNN的这种顺序的结构设计就有问题，你们这些跟随者即使做再多RNN局部优化的尝试，也多是隔靴挠痒，意义不大。只有从根本上改变，才能实现真正的突破，比如注意力机制）</p><hr><p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.</p><p>解读与厚黑：</p><p>众所周知，在翻译过程中，不同的英文对中文的依赖程度不同。还有文本在表征语义时，当前词需要获取的信息可能来自距离很远的另一个词。 注意力机制（Attention mechanisms）通过将序列中任意两个位置之间的距离缩小为一个常量 ，从而解决长距离依赖的问题。这种对依赖关系进行建模的方式，不仅可以很好的适应了我们人脑的认知，还能够实现并行。在大多数情况下，注意机制与循环网络一起使用往往会产生很好的效果。（隐义：你看注意力机制就不一样了吧，把你最根本的问题解决了，并且我还发现Attention搭配上RNN，效果倍棒。方向给你们了，自己看着办！）</p><hr><p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p><p>解读与厚黑：</p><p>Transformer的核心就是使用Attention来绘制输入和输出之间的全局依赖关系，除此之外，还可以使用并行训练的方式大大缩短训练时间。（隐义：咱们这个Transformer内部用的可是Attention，训练快效果好，洋气又时髦，了解一下啊大哥）</p><hr><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet  and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.  In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p><p>解读与厚黑：</p><p>RNN的最大问题在于顺序计算，虽然通过卷积神经网络的方式可以减少顺序计算，实现并行计算。但是模型在学习任意两个位置之间的依赖关系时，所需的操作数随着距离的增长而增长。这就使得模型学习遥远位置之间的依赖关系要更加困难。而在Transformer中，学习任意两个位置之间的依赖关系所需的操作数被减少为恒定次数。所以Transformer要相比于卷积的形式就来的更加有效率。（隐义：虽然你卷积可以并行，但是你有长距离难计算的问题啊，我的Transformer就不同了，不仅具有你并行的好处，还能轻松计算任意位置间的依赖关系。卷积你不太行啊！）</p><hr><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.</p><p>解读与厚黑：</p><p>自注意机制作为注意机制中的一种，将单个序列的不同位置联系起来，以计算序列的表示。同时自注意机制已成功地应用于各种任务中，包括阅读理解、抽象摘要和文本隐含等。（隐义：自注意力机制对各种NLP任务的适用性都挺好，东西不错，Transformer也用用。）</p><hr><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks</p><p>直接厚黑：</p><p>End-to-end memory networks也是个好东西，借鉴一下。</p><hr><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models.</p><p>解读与厚黑：</p><p>Transformer 是第一个完全依赖于Self-attention来计算其输入和输出表示的transduction model，而不使用序列对齐的RNN或卷积神经网络。（隐义：Transformer结合了Self-attention和End-to-end memory networks的好处，可以说是开创性尝试，下面正式开吹。）</p><hr><p>参考博客：</p><p>LSTM 已死，事实真是这样吗？</p><p><a href="https://view.inews.qq.com/a/20220325A03LNX00">https://view.inews.qq.com/a/20220325A03LNX00</a></p><p>Attention is all you need 详解Transformer</p><p><a href="https://www.cnblogs.com/zhanghaiyan/p/11079504.html">https://www.cnblogs.com/zhanghaiyan/p/11079504.html</a></p><p>Attention is all you need博客</p><p><a href="https://blog.csdn.net/sinat_33741547/article/details/85884052">https://blog.csdn.net/sinat_33741547/article/details/85884052</a></p><p>论文解读:Attention is All you need</p><p><a href="https://zhuanlan.zhihu.com/p/46990010">https://zhuanlan.zhihu.com/p/46990010</a></p><p>长依赖问题：</p><p><a href="https://zhuanlan.zhihu.com/p/69704935">https://zhuanlan.zhihu.com/p/69704935</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试算法100问---两数相加</title>
      <link href="/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
      <url>/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="面试算法100问—两数相加"><a href="#面试算法100问—两数相加" class="headerlink" title="面试算法100问—两数相加"></a>面试算法100问—两数相加</h1><p>面试官：Samira</p><p>应聘者：Aqua</p><p>Samira：”接下来我们做道题吧！“</p><p>Aqua（紧张）：”可以的。“</p><p>Samira在视频面试的做题面板上开始不紧不慢的敲击键盘。Aqua本以为Samira会敲出一道完整且表述清晰的题目，可惜他多想了，Samira只是简单给了一个数组nums&#x3D;[2,7,11,15]和target&#x3D;9，然后开始口述。</p><p>Samira：”这个数组nums和target可以看到吧，你现在要在nums中找到和为target的两个整数，然后返回这两个整数的索引。“</p><p>Aqua直接满脸问号，第一次面本来就紧张，还是口述题目，口述也就算了，关键面试官还带口音。直接当场麻掉。</p><p>Samira（很轻松）：”你不用那么紧张，如果有思路可以先聊聊看。“</p><p>Aqua思考了一会，好像回忆起来了，这个听着有点像那个两数相加。</p><p>Aqua（保持假笑）：“请问还有其他要求吗？比如每个nums只有一个答案，同时nums中同一个元素在答案中不能重复出现。”</p><p>Samira：“很好，要求如你所说，并且你可以按任意顺序返回答案。”</p><p>Aqua一边在心里抱怨，为什么不讲清楚，面试官都是玩猜的吗？一边在心里亲切的问候着Samira的家人。然后又继续想了一分钟</p><p>下边附加上LeetCode原题描述：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。</span><br><span class="line">你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</span><br><span class="line">你可以按任意顺序返回答案。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line">输入：nums = [<span class="number">2</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">15</span>], target = <span class="number">9</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">解释：因为 nums[<span class="number">0</span>] + nums[<span class="number">1</span>] == <span class="number">9</span> ，返回 [<span class="number">0</span>, <span class="number">1</span>] 。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">2</span>：</span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>], target = <span class="number">6</span></span><br><span class="line">输出：[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">示例 <span class="number">3</span>：</span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">3</span>], target = <span class="number">6</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406203823.png"></p><p>Aqua：“我需要从nums中找到，唯一的一对索引答案[pos1,pos2]，假设我是按照顺序输出的，即pos1在pos2之前，如果如果能够找到pos2，那么只需要在pos2位置之前的元素中寻找pos1即可。这样我便可以遍历nums，假设当前遍历到nums[i]的索引 i （0&lt;&#x3D;i&lt;n）是满足题意的pos2，那么只需看在索引 i 之前是否存在target-nums[i]，若存在，这元素target-nums[i]的索引 j 即为pos1，索引 i 即为pos2。现在的问题是怎么找target-nums[i]，难道从0到 i 再遍历一遍nums，这样去寻找target-nums[i]这样的话时间复杂度就是O(n)。而寻找pos2的过程的时间复杂度为O(n)，这样总的时间复杂度就是O(n*n)。”</p><p>Simara：“可以再降低一下时间复杂度吗？”</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406212608.png"></p><p>Aqua：“那就牺牲空间来减少时间吧，如果我能够在每次遍历到nums[i]的时候都能够把nums[i]之前的元素存储在HashMap中，把nums[j]当做key，把索引j作为value，0&lt;&#x3D;j&lt;i。这样在HashMap中寻找target-nums[i]也就是寻找pos1的时间复杂度为O(1)，寻找pos2的过程中需要遍历nums时间复杂度为O(n)，所以总的时间复杂度为O(n)。同时，因为这个过程用到了HashMap，所以空间复杂度为O(n)。“</p><p>Simara：“你是想把所有的nums中元素和其对应的索引一次性全部放到HashMap中，还是在遍历过程中进行动态扩充HashMap？”</p><p>Aqua：“进行动态扩充吧，如果我在遍历nums的过程其实就是寻找pos2的过程，如果遍历到的nums[i]的索引 i 不是答案中pos2，也就是在nums[i]之前不存在target-nums[i]，那么说明pos2还在当前 i 的后面，也就是说这个nums[i] 和索引 i 是需要加入到HashMap中的，那么直接加入HashMap。”</p><p>Simara：“那按照你的思路实现一下代码吧！”</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Solution</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">    <span class="keyword">if</span>(nums == <span class="literal">null</span> || nums.length == <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">0</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span>[] pos = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">    HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Integer,Integer&gt;();</span><br><span class="line">    <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> nums.length;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(map.containsKey(target-nums[i]))&#123;</span><br><span class="line">    pos[<span class="number">0</span>] = map.get(target-nums[i]);</span><br><span class="line">    pos[<span class="number">1</span>] = i;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    map.put(nums[i], i);</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">return</span> pos;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 面试算法100问 </category>
          
          <category> HashMap </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试算法题 </tag>
            
            <tag> 简单算法题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>启程算法之旅</title>
      <link href="/2022/03/26/%E5%90%AF%E7%A8%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E6%97%85/"/>
      <url>/2022/03/26/%E5%90%AF%E7%A8%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E6%97%85/</url>
      
        <content type="html"><![CDATA[<h1 id="博客说明"><a href="#博客说明" class="headerlink" title="博客说明"></a>博客说明</h1><p>自己一直有记笔记的习惯，感觉这样可以很好减少重复学习的时间。虽然现在CSDN和知乎都可以通过博客的方式来记录学习内容，但是总感觉他们那个书写界面不太适合自己。所以平常都是通过Typora记录在本地，随着记录的内容不断增多，感觉总是在本地和手机之间传来传去有点不太方便。所以就想做个个人博客吧，这样既能继续保留Typora写文档的习惯，还能把学的东西系统化的展示出来，同时也能看的方便。这么一琢磨，确实是时候做一做了。</p><p>经过两天的学习，终于把这个博客搞完了。除了界面的美化不能和专业前端相比之外，现有的功能已经基本满足个人的使用需求。根据目前的情况，以后也会不定期的更新博客，后续内容主要分两方面：第一方面主要是一些常见的机器学习算法的应用和理论（LR，SVM，随机森林，决策树，GBDT，XGboost，LightGBM，贝叶斯网络，HMM，CRF和聚类等）。对于深度学习这一块，因为自己主要搞NLP方向，所以会集中讲一些深度学习框架应用（CNN，RNN，Word2Vector，LSTM，Transformer，BERT，RoBert，XLBert和XLNet等）。最后是希望自己可以保证精产还能多产。当然最重要的是希望通过写博客这样一种方式，可以让自己的学习过程能更扎实一些。也让自己对技术的掌握更熟练一些。毕竟技术的尽头还是技术。</p><h2 id="Hexo个人常用命令"><a href="#Hexo个人常用命令" class="headerlink" title="Hexo个人常用命令"></a>Hexo个人常用命令</h2><p>为了减少重复搜索，还是记下来比较好</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s <span class="comment">#启动本地静态页面</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g <span class="comment">#生成静态文件</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d <span class="comment">#部署到远程站点</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> Hexo使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
            <tag> Hexo框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/25/hello-world/"/>
      <url>/2022/03/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
            <tag> 个人建设 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
