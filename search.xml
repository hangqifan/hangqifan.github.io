<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Python六式---判断和循环</title>
      <link href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%88%A4%E6%96%AD%E5%92%8C%E5%BE%AA%E7%8E%AF/"/>
      <url>/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%88%A4%E6%96%AD%E5%92%8C%E5%BE%AA%E7%8E%AF/</url>
      
        <content type="html"><![CDATA[<h1 id="Python六式—-判断和循环"><a href="#Python六式—-判断和循环" class="headerlink" title="Python六式—-判断和循环"></a>Python六式—-判断和循环</h1><h2 id="判断（if）语句"><a href="#判断（if）语句" class="headerlink" title="判断（if）语句"></a>判断（if）语句</h2><h3 id="if-判断语句基本语法"><a href="#if-判断语句基本语法" class="headerlink" title="if 判断语句基本语法"></a>if 判断语句基本语法</h3><p>在 <code>Python</code> 中，<strong>if 语句</strong> 就是用来进行判断的，格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 要判断的条件:</span><br><span class="line">    条件成立时，要做的事情</span><br><span class="line">    ……</span><br></pre></td></tr></table></figure><blockquote><p>注意：代码的缩进为一个 <code>tab</code> 键，或者 <strong>4</strong> 个空格 —— <strong>建议使用空格</strong></p><ul><li>在 Python 开发中，Tab 和空格不要混用！</li></ul></blockquote><h3 id="else-处理条件不满足的情况"><a href="#else-处理条件不满足的情况" class="headerlink" title="else 处理条件不满足的情况"></a>else 处理条件不满足的情况</h3><p><strong>思考</strong></p><p>在使用 <code>if</code> 判断时，只能做到满足条件时要做的事情。那如果需要在 <strong>不满足条件的时候</strong>，做某些事情，该如何做呢？</p><p><strong>答案</strong></p><p><code>else</code>，格式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 要判断的条件:</span><br><span class="line">    条件成立时，要做的事情</span><br><span class="line">    ……</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    条件不成立时，要做的事情</span><br><span class="line">    ……</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：</p><ul><li><code>if</code> 和 <code>else</code> 语句以及各自的缩进部分共同是一个 <strong>完整的代码块</strong></li></ul><h3 id="if-语句进阶elif"><a href="#if-语句进阶elif" class="headerlink" title="if 语句进阶elif"></a>if 语句进阶elif</h3><ul><li>在开发中，使用 <code>if</code> 可以 <strong>判断条件</strong></li><li>使用 <code>else</code> 可以处理 <strong>条件不成立</strong> 的情况</li><li>但是，如果希望 <strong>再增加一些条件</strong>，<strong>条件不同，需要执行的代码也不同</strong> 时，就可以使用 <code>elif</code> </li><li>语法格式如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件<span class="number">1</span>:</span><br><span class="line">    条件<span class="number">1</span>满足执行的代码</span><br><span class="line">    ……</span><br><span class="line"><span class="keyword">elif</span> 条件<span class="number">2</span>:</span><br><span class="line">    条件<span class="number">2</span>满足时，执行的代码</span><br><span class="line">    ……</span><br><span class="line"><span class="keyword">elif</span> 条件<span class="number">3</span>:</span><br><span class="line">    条件<span class="number">3</span>满足时，执行的代码</span><br><span class="line">    ……</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    以上条件都不满足时，执行的代码</span><br><span class="line">    ……</span><br></pre></td></tr></table></figure><ul><li>对比逻辑运算符的代码</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> 条件<span class="number">1</span> <span class="keyword">and</span> 条件<span class="number">2</span>:</span><br><span class="line">    条件<span class="number">1</span>满足 并且 条件<span class="number">2</span>满足 执行的代码</span><br><span class="line">    ……</span><br></pre></td></tr></table></figure><p><strong>注意</strong></p><ol><li><code>elif</code> 和 <code>else</code> 都必须和 <code>if</code> 联合使用，而不能单独使用</li><li>可以将 <code>if</code>、<code>elif</code> 和 <code>else</code> 以及各自缩进的代码，看成一个 <strong>完整的代码块</strong></li></ol><h2 id="随机数的处理"><a href="#随机数的处理" class="headerlink" title="随机数的处理"></a>随机数的处理</h2><ul><li>在 <code>Python</code> 中，要使用随机数，首先需要导入 <strong>随机数</strong> 的 <strong>模块</strong> —— “工具包”</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><ul><li><p>导入模块后，可以直接在 <strong>模块名称</strong> 后面敲一个 <code>.</code> 然后按 <code>Tab</code> 键，会提示该模块中包含的所有函数</p></li><li><p><code>random.randint(a, b)</code> ，返回 <code>[a, b]</code> 之间的整数，包含 <code>a</code> 和 <code>b</code></p></li><li>例如：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">random.randint(<span class="number">12</span>, <span class="number">20</span>)  <span class="comment"># 生成的随机数n: 12 &lt;= n &lt;= 20   </span></span><br><span class="line">random.randint(<span class="number">20</span>, <span class="number">20</span>)  <span class="comment"># 结果永远是 20   </span></span><br><span class="line">random.randint(<span class="number">20</span>, <span class="number">10</span>)  <span class="comment"># 该语句是错误的，下限必须小于上限</span></span><br></pre></td></tr></table></figure><h2 id="循环"><a href="#循环" class="headerlink" title="循环"></a>循环</h2><h3 id="while语句基本语法"><a href="#while语句基本语法" class="headerlink" title="while语句基本语法"></a>while语句基本语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">初始条件设置 —— 通常是重复执行的 计数器</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> 条件(判断 计数器 是否达到 目标次数):</span><br><span class="line">    条件满足时，做的事情<span class="number">1</span></span><br><span class="line">    条件满足时，做的事情<span class="number">2</span></span><br><span class="line">    条件满足时，做的事情<span class="number">3</span></span><br><span class="line">    ...(省略)...</span><br><span class="line">    </span><br><span class="line">    处理条件(计数器 + <span class="number">1</span>)</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：</p><ul><li><code>while</code> 语句以及缩进部分是一个 <strong>完整的代码块</strong></li></ul><h3 id="Python-中的计数方法"><a href="#Python-中的计数方法" class="headerlink" title="Python 中的计数方法"></a>Python 中的计数方法</h3><p>常见的计数方法有两种，可以分别称为：</p><ul><li><strong>自然计数法</strong>（从 <code>1</code> 开始）—— 更符合人类的习惯</li><li><strong>程序计数法</strong>（从 <code>0</code> 开始）—— 几乎所有的程序语言都选择从 0 开始计数</li></ul><p>因此，大家在编写程序时，应该尽量养成习惯：<strong>除非需求的特殊要求，否则 循环 的计数都从 0 开始</strong></p><h3 id="break-和-continue"><a href="#break-和-continue" class="headerlink" title="break 和 continue"></a>break 和 continue</h3><blockquote><p><code>break</code> 和 <code>continue</code> 是专门在循环中使用的关键字</p></blockquote><ul><li><code>break</code> <strong>某一条件满足时</strong>，退出循环，不再执行后续重复的代码</li><li><code>continue</code> <strong>某一条件满足时</strong>，不执行后续重复的代码</li></ul><blockquote><p><code>break</code> 和 <code>continue</code> 只针对 <strong>当前所在循环</strong> 有效</p></blockquote><h2 id="字符串中的转义字符"><a href="#字符串中的转义字符" class="headerlink" title="字符串中的转义字符"></a><strong>字符串中的转义字符</strong></h2><ul><li><code>\t</code> 在控制台输出一个 <strong>制表符</strong>，协助在输出文本时 <strong>垂直方向</strong> 保持对齐</li><li><code>\n</code> 在控制台输出一个 <strong>换行符</strong></li></ul><blockquote><p><strong>制表符</strong> 的功能是在不使用表格的情况下在 <strong>垂直方向</strong> 按列对齐文本</p></blockquote><div class="table-container"><table><thead><tr><th>转义字符</th><th>描述</th></tr></thead><tbody><tr><td>\\</td><td>反斜杠符号</td></tr><tr><td>\‘</td><td>单引号</td></tr><tr><td>\“</td><td>双引号</td></tr><tr><td>\n</td><td>换行</td></tr><tr><td>\t</td><td>横向制表符</td></tr><tr><td>\r</td><td>回车</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> python基础知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python六式 </tag>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python六式---变量的命名和运算符</title>
      <link href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%8F%98%E9%87%8F%E7%9A%84%E5%91%BD%E5%90%8D%E5%92%8C%E8%BF%90%E7%AE%97%E7%AC%A6/"/>
      <url>/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E5%8F%98%E9%87%8F%E7%9A%84%E5%91%BD%E5%90%8D%E5%92%8C%E8%BF%90%E7%AE%97%E7%AC%A6/</url>
      
        <content type="html"><![CDATA[<h1 id="Python六式—-变量的命名和运算符"><a href="#Python六式—-变量的命名和运算符" class="headerlink" title="Python六式—-变量的命名和运算符"></a>Python六式—-变量的命名和运算符</h1><p>任何语言的程序员，编写出符合规范的代码，是开始程序生涯的第一步</p><h2 id="变量的命名"><a href="#变量的命名" class="headerlink" title="变量的命名"></a>变量的命名</h2><h3 id="标识符和关键字"><a href="#标识符和关键字" class="headerlink" title="标识符和关键字"></a>标识符和关键字</h3><h4 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h4><p>标示符就是程序员定义的 <strong>变量名</strong>、<strong>函数名</strong></p><ul><li>标示符可以由 <strong>字母</strong>、<strong>下划线</strong> 和 <strong>数字</strong> 组成</li><li><strong>不能以数字开头</strong></li><li><strong>不能与关键字重名</strong></li></ul><h4 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h4><ul><li><strong>关键字</strong> 就是在 <code>Python</code> 内部已经使用的标识符</li><li><strong>关键字</strong> 具有特殊的功能和含义</li><li>开发者 <strong>不允许定义和关键字相同的名字的标示符</strong></li></ul><p>通过以下命令可以查看 <code>Python</code> 中的关键字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> keyword</span><br><span class="line">In [<span class="number">2</span>]: <span class="built_in">print</span>(keyword.kwlist)</span><br></pre></td></tr></table></figure><h3 id="变量的命名规则"><a href="#变量的命名规则" class="headerlink" title="变量的命名规则"></a>变量的命名规则</h3><p><strong>命名规则</strong> 可以被视为一种 <strong>惯例</strong>，并无绝对与强制，目的是为了 <strong>增加代码的识别和可读性</strong></p><p><strong>注意</strong> <code>Python</code> 中的 <strong>标识符</strong> 是 <strong>区分大小写的</strong></p><h4 id="下划线命名法（我喜欢这个）"><a href="#下划线命名法（我喜欢这个）" class="headerlink" title="下划线命名法（我喜欢这个）"></a>下划线命名法（我喜欢这个）</h4><ol><li><p>在定义变量时，为了保证代码格式，<code>=</code> 的左右应该各保留一个空格</p></li><li><p>在 <code>Python</code> 中，如果 <strong>变量名</strong> 需要由 <strong>二个</strong> 或 <strong>多个单词</strong> 组成时，可以按照以下方式命名</p><ol><li>每个单词都使用小写字母</li><li>单词与单词之间使用 <strong><code>_</code>下划线</strong> 连接</li></ol><ul><li>例如：<code>first_name</code>、<code>last_name</code>、<code>qq_number</code>、<code>qq_password</code></li></ul></li></ol><h4 id="驼峰命名法"><a href="#驼峰命名法" class="headerlink" title="驼峰命名法"></a>驼峰命名法</h4><ul><li>当 <strong>变量名</strong> 是由二个或多个单词组成时，还可以利用驼峰命名法来命名</li><li><strong>小驼峰式命名法</strong><ul><li>第一个单词以小写字母开始，后续单词的首字母大写</li><li>例如：<code>firstName</code>、<code>lastName</code></li></ul></li><li><strong>大驼峰式命名法</strong><ul><li>每一个单词的首字母都采用大写字母</li><li>例如：<code>FirstName</code>、<code>LastName</code>、<code>CamelCase</code> </li></ul></li></ul><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220422145701.jpg" alt></p><h2 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h2><h3 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h3><p>是完成基本的算术运算使用的符号，用来处理四则运算</p><div class="table-container"><table><thead><tr><th style="text-align:center">运算符</th><th style="text-align:center">描述</th><th>实例</th></tr></thead><tbody><tr><td style="text-align:center">+</td><td style="text-align:center">加</td><td>10 + 20 = 30</td></tr><tr><td style="text-align:center">-</td><td style="text-align:center">减</td><td>10 - 20 = -10</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">乘</td><td>10 * 20 = 200</td></tr><tr><td style="text-align:center">/</td><td style="text-align:center">除</td><td>10 / 20 = 0.5</td></tr><tr><td style="text-align:center">//</td><td style="text-align:center">取整除</td><td>返回除法的整数部分（商） 9 // 2 输出结果 4</td></tr><tr><td style="text-align:center">%</td><td style="text-align:center">取余数</td><td>返回除法的余数 9 % 2 = 1</td></tr><tr><td style="text-align:center">**</td><td style="text-align:center">幂</td><td>又称次方、乘方，2 ** 3 = 8</td></tr></tbody></table></div><h3 id="比较（关系）运算符"><a href="#比较（关系）运算符" class="headerlink" title="比较（关系）运算符"></a>比较（关系）运算符</h3><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>==</td><td>检查两个操作数的值是否 <strong>相等</strong>，如果是，则条件成立，返回 True</td></tr><tr><td>!=</td><td>检查两个操作数的值是否 <strong>不相等</strong>，如果是，则条件成立，返回 True</td></tr><tr><td>&gt;</td><td>检查左操作数的值是否 <strong>大于</strong> 右操作数的值，如果是，则条件成立，返回 True</td></tr><tr><td>&lt;</td><td>检查左操作数的值是否 <strong>小于</strong> 右操作数的值，如果是，则条件成立，返回 True</td></tr><tr><td>&gt;=</td><td>检查左操作数的值是否 <strong>大于或等于</strong> 右操作数的值，如果是，则条件成立，返回 True</td></tr><tr><td>&lt;=</td><td>检查左操作数的值是否 <strong>小于或等于</strong> 右操作数的值，如果是，则条件成立，返回 True</td></tr></tbody></table></div><blockquote><p>Python 2.x 中判断 <strong>不等于</strong> 还可以使用 <code>&lt;&gt;</code> 运算符</p><p><code>!=</code> 在 Python 2.x 中同样可以用来判断 <strong>不等于</strong></p></blockquote><h3 id="逻辑运算符"><a href="#逻辑运算符" class="headerlink" title="逻辑运算符"></a>逻辑运算符</h3><div class="table-container"><table><thead><tr><th>运算符</th><th>逻辑表达式</th><th>描述</th></tr></thead><tbody><tr><td>and</td><td>x and y</td><td>只有 x 和 y 的值都为 True，才会返回 True。否则只要 x 或者 y 有一个值为 False，就返回 False</td></tr><tr><td>or</td><td>x or y</td><td>只要 x 或者 y 有一个值为 True，就返回 True。只有 x 和 y 的值都为 False，才会返回 False</td></tr><tr><td>not</td><td>not x</td><td>如果 x 为 True，返回 False。如果 x 为 False，返回 True</td></tr></tbody></table></div><h3 id="赋值运算符"><a href="#赋值运算符" class="headerlink" title="赋值运算符"></a>赋值运算符</h3><ul><li>在 Python 中，使用 <code>=</code> 可以给变量赋值</li><li>在算术运算时，为了简化代码的编写，<code>Python</code> 还提供了一系列的 与 <strong>算术运算符</strong> 对应的 <strong>赋值运算符</strong></li><li>注意：<strong>赋值运算符中间不能使用空格</strong></li></ul><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th><th>实例</th></tr></thead><tbody><tr><td>=</td><td>简单的赋值运算符</td><td>c = a + b 将 a + b 的运算结果赋值为 c</td></tr><tr><td>+=</td><td>加法赋值运算符</td><td>c += a 等效于 c = c + a</td></tr><tr><td>-=</td><td>减法赋值运算符</td><td>c -= a 等效于 c = c - a</td></tr><tr><td>*=</td><td>乘法赋值运算符</td><td>c <em>= a 等效于 c = c </em> a</td></tr><tr><td>/=</td><td>除法赋值运算符</td><td>c /= a 等效于 c = c / a</td></tr><tr><td>//=</td><td>取整除赋值运算符</td><td>c //= a 等效于 c = c // a</td></tr><tr><td>%=</td><td>取 <strong>模</strong> (余数)赋值运算符</td><td>c %= a 等效于 c = c % a</td></tr><tr><td>**=</td><td>幂赋值运算符</td><td>c <strong> = a 等效于 c = c </strong> a</td></tr></tbody></table></div><h3 id="运算符的优先级"><a href="#运算符的优先级" class="headerlink" title="运算符的优先级"></a>运算符的优先级</h3><p>以下表格的算数优先级由高到最低顺序排列</p><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>**</td><td>幂 (最高优先级)</td></tr><tr><td>* / % //</td><td>乘、除、取余数、取整除</td></tr><tr><td>+ -</td><td>加法、减法</td></tr><tr><td>&lt;= &lt; &gt; &gt;=</td><td>比较运算符</td></tr><tr><td>== !=</td><td>等于运算符</td></tr><tr><td>= %= /= //= -= += <em>= *</em>=</td><td>赋值运算符</td></tr><tr><td>not or and</td><td>逻辑运算符</td></tr></tbody></table></div>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> python基础知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python六式 </tag>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python六式---算数运算符和变量的基本使用</title>
      <link href="/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6%E5%92%8C%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"/>
      <url>/2022/04/22/Python%E5%85%AD%E5%BC%8F---%E7%AE%97%E6%95%B0%E8%BF%90%E7%AE%97%E7%AC%A6%E5%92%8C%E5%8F%98%E9%87%8F%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<h1 id="Python六式—-算数运算符和变量的基本使用"><a href="#Python六式—-算数运算符和变量的基本使用" class="headerlink" title="Python六式—-算数运算符和变量的基本使用"></a>Python六式—-算数运算符和变量的基本使用</h1><h2 id="算数运算符"><a href="#算数运算符" class="headerlink" title="算数运算符"></a>算数运算符</h2><h3 id="常用的算数运算符"><a href="#常用的算数运算符" class="headerlink" title="常用的算数运算符"></a>常用的算数运算符</h3><div class="table-container"><table><thead><tr><th style="text-align:center">运算符</th><th style="text-align:center">描述</th><th>实例</th></tr></thead><tbody><tr><td style="text-align:center">+</td><td style="text-align:center">加</td><td>10 + 20 = 30</td></tr><tr><td style="text-align:center">-</td><td style="text-align:center">减</td><td>10 - 20 = -10</td></tr><tr><td style="text-align:center">*</td><td style="text-align:center">乘</td><td>10 * 20 = 200</td></tr><tr><td style="text-align:center">/</td><td style="text-align:center">除</td><td>10 / 20 = 0.5</td></tr><tr><td style="text-align:center">//</td><td style="text-align:center">取整除</td><td>返回除法的整数部分（商） 9 // 2 输出结果 4</td></tr><tr><td style="text-align:center">%</td><td style="text-align:center">取余数</td><td>返回除法的余数 9 % 2 = 1</td></tr><tr><td style="text-align:center">**</td><td style="text-align:center">幂</td><td>又称次方、乘方，2 ** 3 = 8</td></tr></tbody></table></div><p>在 Python 中 <code>*</code> 运算符还可以用于字符串，计算结果就是字符串重复指定次数的结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="string">&quot;-&quot;</span> * <span class="number">50</span></span><br><span class="line">Out[<span class="number">1</span>]: <span class="string">&#x27;----------------------------------------&#x27;</span> </span><br></pre></td></tr></table></figure><h3 id="算数运算符的优先级"><a href="#算数运算符的优先级" class="headerlink" title="算数运算符的优先级"></a>算数运算符的优先级</h3><p>和数学中的运算符的优先级一致，在 Python 中进行数学计算时，同样也是：</p><ul><li><strong>先乘除后加减</strong></li><li>同级运算符是 <strong>从左至右</strong> 计算</li><li>可以使用 <code>()</code> 调整计算的优先级</li></ul><div class="table-container"><table><thead><tr><th>运算符</th><th>描述</th></tr></thead><tbody><tr><td>**</td><td>幂 (最高优先级)</td></tr><tr><td>* / % //</td><td>乘、除、取余数、取整除</td></tr><tr><td>+ -</td><td>加法、减法</td></tr></tbody></table></div><h2 id="变量的基本使用"><a href="#变量的基本使用" class="headerlink" title="变量的基本使用"></a>变量的基本使用</h2><h3 id="变量定义"><a href="#变量定义" class="headerlink" title="变量定义"></a>变量定义</h3><ul><li>在 Python 中，每个变量 <strong>在使用前都必须赋值</strong>，变量 <strong>赋值以后</strong> 该变量 <strong>才会被创建</strong></li><li>等号（=）用来给变量赋值<ul><li><code>=</code> 左边是一个变量名</li><li><code>=</code> 右边是存储在变量中的值</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">变量名 = 值</span><br></pre></td></tr></table></figure><h3 id="变量的类型"><a href="#变量的类型" class="headerlink" title="变量的类型"></a>变量的类型</h3><p>在内存中创建一个变量，会包括：</p><ol><li>变量的名称</li><li>变量保存的数据</li><li>变量存储数据的类型</li><li>变量的地址（标示</li></ol><p>在 <code>Python</code> 中定义变量是 <strong>不需要指定类型</strong>（在其他很多高级语言中都需要）</p><p>数据类型可以分为 <strong>数字型</strong> 和 <strong>非数字型</strong></p><p>1、数字型</p><ul><li>整型 (<code>int</code>)</li><li>浮点型（<code>float</code>）</li><li>布尔型（<code>bool</code>） <ul><li>真 <code>True</code> <code>非 0 数</code> —— <strong>非零即真</strong></li><li>假 <code>False</code> <code>0</code></li></ul></li><li>复数型 (<code>complex</code>)<ul><li>主要用于科学计算，例如：平面场问题、波动问题、电感电容等问题</li></ul></li></ul><p>2、非数字型</p><ul><li>字符串</li><li>列表</li><li>元组</li><li>字典</li></ul><p>使用 <code>type</code> 函数可以查看一个变量的类型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="built_in">type</span>(name)</span><br></pre></td></tr></table></figure><h3 id="不同类型变量之间的计算"><a href="#不同类型变量之间的计算" class="headerlink" title="不同类型变量之间的计算"></a>不同类型变量之间的计算</h3><h4 id="数字型变量-之间可以直接计算"><a href="#数字型变量-之间可以直接计算" class="headerlink" title="数字型变量 之间可以直接计算"></a><strong>数字型变量</strong> 之间可以直接计算</h4><ul><li>在 Python 中，两个数字型变量是可以直接进行 算数运算的</li><li>如果变量是 <code>bool</code> 型，在计算时<ul><li><code>True</code> 对应的数字是 <code>1</code></li><li><code>False</code> 对应的数字是 <code>0</code></li></ul></li></ul><h4 id="字符串变量-之间使用-拼接字符串"><a href="#字符串变量-之间使用-拼接字符串" class="headerlink" title="字符串变量 之间使用 + 拼接字符串"></a><strong>字符串变量</strong> 之间使用 <code>+</code> 拼接字符串</h4><p>在 Python 中，字符串之间可以使用 <code>+</code> 拼接生成新的字符串</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: first_name = <span class="string">&quot;三&quot;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: last_name = <span class="string">&quot;张&quot;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: first_name + last_name</span><br><span class="line">Out[<span class="number">3</span>]: <span class="string">&#x27;三张&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="字符串变量-可以和-整数-使用-重复拼接相同的字符串"><a href="#字符串变量-可以和-整数-使用-重复拼接相同的字符串" class="headerlink" title="字符串变量 可以和 整数 使用 * 重复拼接相同的字符串"></a><strong>字符串变量</strong> 可以和 <strong>整数</strong> 使用 <code>*</code> 重复拼接相同的字符串</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="string">&quot;-&quot;</span> * <span class="number">50</span></span><br><span class="line">Out[<span class="number">1</span>]: <span class="string">&#x27;--------------------------------------------------&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="数字型变量-和-字符串-之间-不能进行其他计算"><a href="#数字型变量-和-字符串-之间-不能进行其他计算" class="headerlink" title="数字型变量 和 字符串 之间 不能进行其他计算"></a><strong>数字型变量</strong> 和 <strong>字符串</strong> 之间 <strong>不能进行其他计算</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: first_name = <span class="string">&quot;zhang&quot;</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: x = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: x + first_name</span><br><span class="line">---------------------------------------------------------------------------</span><br><span class="line">TypeError: unsupported operand <span class="built_in">type</span>(s) <span class="keyword">for</span> +: <span class="string">&#x27;int&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;str&#x27;</span></span><br><span class="line">类型错误：`+` 不支持的操作类型：`<span class="built_in">int</span>` 和 `<span class="built_in">str</span>`</span><br></pre></td></tr></table></figure><h3 id="变量的输入"><a href="#变量的输入" class="headerlink" title="变量的输入"></a>变量的输入</h3><h4 id="输出函数"><a href="#输出函数" class="headerlink" title="输出函数"></a>输出函数</h4><div class="table-container"><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>print(x)</td><td>将 x 输出到控制台</td></tr><tr><td>type(x)</td><td>查看 x 的变量类型</td></tr></tbody></table></div><h4 id="input-函数实现键盘输入"><a href="#input-函数实现键盘输入" class="headerlink" title="input 函数实现键盘输入"></a>input 函数实现键盘输入</h4><ul><li>在 Python 中可以使用 <code>input</code> 函数从键盘等待用户的输入</li><li>用户输入的 <strong>任何内容</strong> Python 都认为是一个 <strong>字符串</strong></li><li>语法如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">字符串变量 = <span class="built_in">input</span>(<span class="string">&quot;提示信息：&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="类型转换函数"><a href="#类型转换函数" class="headerlink" title="类型转换函数"></a>类型转换函数</h4><div class="table-container"><table><thead><tr><th>函数</th><th>说明</th></tr></thead><tbody><tr><td>int(x)</td><td>将 x 转换为一个整数</td></tr><tr><td>float(x)</td><td>将 x 转换到一个浮点数</td></tr></tbody></table></div><h4 id="变量输入演练-——-超市买苹果增强版"><a href="#变量输入演练-——-超市买苹果增强版" class="headerlink" title="变量输入演练 —— 超市买苹果增强版"></a>变量输入演练 —— 超市买苹果增强版</h4><p><strong>需求</strong></p><ul><li><strong>收银员输入</strong> 苹果的价格，单位：<strong>元／斤</strong></li><li><strong>收银员输入</strong> 用户购买苹果的重量，单位：<strong>斤</strong></li><li>计算并且 <strong>输出</strong> 付款金额</li></ul><h5 id="演练方式-1"><a href="#演练方式-1" class="headerlink" title="演练方式 1"></a>演练方式 1</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 输入苹果单价</span></span><br><span class="line">price_str = <span class="built_in">input</span>(<span class="string">&quot;请输入苹果价格：&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 要求苹果重量</span></span><br><span class="line">weight_str = <span class="built_in">input</span>(<span class="string">&quot;请输入苹果重量：&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 计算金额</span></span><br><span class="line"><span class="comment"># 1&gt; 将苹果单价转换成小数</span></span><br><span class="line">price = <span class="built_in">float</span>(price_str)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2&gt; 将苹果重量转换成小数</span></span><br><span class="line">weight = <span class="built_in">float</span>(weight_str)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3&gt; 计算付款金额</span></span><br><span class="line">money = price * weight</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(money)</span><br></pre></td></tr></table></figure><p><strong>提问</strong></p><ol><li>演练中，针对 <strong>价格</strong> 定义了几个变量？<ul><li><strong>两个</strong></li><li><code>price_str</code> 记录用户输入的价格字符串</li><li><code>price</code> 记录转换后的价格数值</li></ul></li><li><strong>思考</strong> —— 如果开发中，需要用户通过控制台 输入 <strong>很多个 数字</strong>，针对每一个数字都要定义两个变量，<strong>方便吗</strong>？ </li></ol><h5 id="演练方式-2-——-买苹果改进版"><a href="#演练方式-2-——-买苹果改进版" class="headerlink" title="演练方式 2 —— 买苹果改进版"></a>演练方式 2 —— 买苹果改进版</h5><ol><li><strong>定义</strong> 一个 <strong>浮点变量</strong> 接收用户输入的同时，就使用 <code>float</code> 函数进行转换</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">price = <span class="built_in">float</span>(<span class="built_in">input</span>(<span class="string">&quot;请输入价格:&quot;</span>))</span><br></pre></td></tr></table></figure><ul><li>改进后的好处：</li></ul><ol><li>节约空间，只需要为一个变量分配空间</li><li>起名字方便，不需要为中间变量起名字</li></ol><ul><li>改进后的“缺点”：</li></ul><ol><li>初学者需要知道，两个函数能够嵌套使用，稍微有一些难度</li></ol><p><strong>提示</strong></p><ul><li>如果输入的不是一个数字，程序执行时会出错，有关数据转换的高级话题，后续会讲！</li></ul><h4 id="变量的格式化输出"><a href="#变量的格式化输出" class="headerlink" title="变量的格式化输出"></a>变量的格式化输出</h4><blockquote><p>苹果单价 <code>9.00</code> 元／斤，购买了 <code>5.00</code> 斤，需要支付 <code>45.00</code> 元</p></blockquote><ul><li>在 Python 中可以使用 <code>print</code> 函数将信息输出到控制台</li><li>如果希望输出文字信息的同时，<strong>一起输出</strong> <strong>数据</strong>，就需要使用到 <strong>格式化操作符</strong></li><li><code>%</code> 被称为 <strong>格式化操作符</strong>，专门用于处理字符串中的格式<ul><li>包含 <code>%</code> 的字符串，被称为 <strong>格式化字符串</strong></li><li><code>%</code> 和不同的 <strong>字符</strong> 连用，<strong>不同类型的数据</strong> 需要使用 <strong>不同的格式化字符</strong></li></ul></li></ul><div class="table-container"><table><thead><tr><th>格式化字符</th><th>含义</th></tr></thead><tbody><tr><td>%s</td><td>字符串</td></tr><tr><td>%d</td><td>有符号十进制整数，<code>%06d</code> 表示输出的整数显示位数，不足的地方使用 <code>0</code> 补全</td></tr><tr><td>%f</td><td>浮点数，<code>%.2f</code> 表示小数点后只显示两位</td></tr><tr><td>%%</td><td>输出 <code>%</code></td></tr></tbody></table></div><ul><li>语法格式如下：</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;格式化字符串&quot;</span> % 变量<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;格式化字符串&quot;</span> % (变量<span class="number">1</span>, 变量<span class="number">2.</span>..))</span><br></pre></td></tr></table></figure><h4 id="格式化输出演练-——-基本练习"><a href="#格式化输出演练-——-基本练习" class="headerlink" title="格式化输出演练 —— 基本练习"></a>格式化输出演练 —— 基本练习</h4><p><strong>需求</strong></p><ol><li>定义字符串变量 <code>name</code>，输出 <strong>我的名字叫 小明，请多多关照！</strong></li><li>定义整数变量 <code>student_no</code>，输出 <strong>我的学号是 000001</strong></li><li>定义小数 <code>price</code>、<code>weight</code>、<code>money</code>，输出 <strong>苹果单价 9.00 元／斤，购买了 5.00 斤，需要支付 45.00 元</strong></li><li>定义一个小数 <code>scale</code>，输出 <strong>数据比例是 10.00%</strong></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;我的名字叫 %s，请多多关照！&quot;</span> % name)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;我的学号是 %06d&quot;</span> % student_no)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;苹果单价 %.02f 元／斤，购买 %.02f 斤，需要支付 %.02f 元&quot;</span> % (price, weight, money))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;数据比例是 %.02f%%&quot;</span> % (scale * <span class="number">100</span>))</span><br></pre></td></tr></table></figure><h4 id><a href="#" class="headerlink" title=" "></a> </h4>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> python基础知识 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python六式 </tag>
            
            <tag> Python基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python读写excel脚本---根据地址提取省市县区</title>
      <link href="/2022/04/18/Python%E8%AF%BB%E5%86%99excel%E8%84%9A%E6%9C%AC---%E6%A0%B9%E6%8D%AE%E5%9C%B0%E5%9D%80%E6%8F%90%E5%8F%96%E7%9C%81%E5%B8%82%E5%8E%BF%E5%8C%BA/"/>
      <url>/2022/04/18/Python%E8%AF%BB%E5%86%99excel%E8%84%9A%E6%9C%AC---%E6%A0%B9%E6%8D%AE%E5%9C%B0%E5%9D%80%E6%8F%90%E5%8F%96%E7%9C%81%E5%B8%82%E5%8E%BF%E5%8C%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="Python读写excel脚本—-根据地址提取省市县区"><a href="#Python读写excel脚本—-根据地址提取省市县区" class="headerlink" title="Python读写excel脚本—-根据地址提取省市县区"></a>Python读写excel脚本—-根据地址提取省市县区</h1><h2 id="pip命令不能安装包问题解决"><a href="#pip命令不能安装包问题解决" class="headerlink" title="pip命令不能安装包问题解决"></a>pip命令不能安装包问题解决</h2><p>不知道是什么原因，自从我的电脑又安装了Anaconda之后，<strong>pip install 第三方库名</strong> 这个命令就不好使了。但是 <strong>conda install 第三方库名</strong> 时好使，而在写本项目的过程中因为要用到cpca这个库，但是conda install cpca也不行，所以经过百般查询，发现可以通过Pycharm界面左下角<strong>Terminal</strong>中输入 <strong>pip install 第三方库名</strong> 来安装需要的库。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220418140345.png" alt></p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220418140404.png" alt></p><h2 id="项目描述"><a href="#项目描述" class="headerlink" title="项目描述"></a>项目描述</h2><p>前几天同学让我帮忙写个脚本，想让我根据扶贫地点中的中的具体地址输出其对应的省份。但问题是有些地址的描述是不够具体的，没有办法直接通过字符串切分直接提取出来省份信息。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220418141024.png" alt></p><p>输出效果如下图所示：</p><p><img src alt><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220418140815.png" alt="微信截图_20220418140724"></p><h2 id="项目解决过程"><a href="#项目解决过程" class="headerlink" title="项目解决过程"></a>项目解决过程</h2><h3 id="库介绍"><a href="#库介绍" class="headerlink" title="库介绍"></a>库介绍</h3><h4 id="cpca库介绍"><a href="#cpca库介绍" class="headerlink" title="cpca库介绍"></a>cpca库介绍</h4><p>一个用于提取简体中文字符串中省，市和区并能够进行映射，检验和简单绘图的python模块。</p><p>举个例子：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[&quot;徐汇区虹漕路461号58号楼5楼&quot;, &quot;泉州市洛江区万安塘西工业区&quot;]</span><br><span class="line">        ↓ 转换</span><br><span class="line">|省    |市   |区    |地址                 |</span><br><span class="line">|上海市|上海市|徐汇区|虹漕路461号58号楼5楼  |</span><br><span class="line">|福建省|泉州市|洛江区|万安塘西工业区        |</span><br></pre></td></tr></table></figure><p>注：“地址”列代表去除了省市区之后的具体地址。</p><p>也可以将大段文本中所有提到的地址提取出来，并且自动将相邻的存在所属关系的地址归并到一条记录中（0.5.5版本新功能）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;分店位于徐汇区虹漕路461号58号楼5楼和泉州市洛江区万安塘西工业区以及南京鼓楼区&quot;</span><br><span class="line">        ↓ 转换</span><br><span class="line">|省    |市   |区    |</span><br><span class="line">|上海市|上海市|徐汇区|</span><br><span class="line">|福建省|泉州市|洛江区|</span><br><span class="line">|江苏省|南京市|鼓楼区|</span><br></pre></td></tr></table></figure><p>其中最重要的一段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cpca</span><br><span class="line">location_str = [<span class="string">&quot;徐汇区虹漕路461号58号楼5楼&quot;</span>, <span class="string">&quot;泉州市洛江区万安塘西工业区&quot;</span>, <span class="string">&quot;北京朝阳区北苑华贸城&quot;</span>]</span><br><span class="line"><span class="comment"># 将location_str中的地址转化为DataFrame格式，pos_sensitive可以获知程序是从字符串的那个位置提取出省市区名的</span></span><br><span class="line">df = cpca.transform(location_str, pos_sensitive=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br></pre></td></tr></table></figure><p>输出结果：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">     省    市    区        地址               adcode        省_pos  市_pos 区_pos</span><br><span class="line">0  上海市  上海市  徐汇区  虹漕路461号58号楼5楼   310104     -1     -1      0</span><br><span class="line">1  福建省  泉州市  洛江区  万安塘西工业区         350504     -1      0      3</span><br><span class="line">2  北京市  市辖区  朝阳区  北苑华贸城            110105     -1     -1      0</span><br></pre></td></tr></table></figure><p>当然这只是最基本的功能，对于其更复杂的句子中包含地址提取，增加索引，以及绘图的功能可以详细阅读如下连接：</p><p><a href="https://github.com/DQinYuan/chinese_province_city_area_mapper">https://github.com/DQinYuan/chinese_province_city_area_mapper</a></p><h4 id="pandas库介绍"><a href="#pandas库介绍" class="headerlink" title="pandas库介绍"></a>pandas库介绍</h4><p>pandas在此项目中主要用来读取excel文件（.xlsx后缀格式文件）以及处理cpca转化的DataFrame格式数据。</p><h4 id="xlrd、openpyxl、xlwt库介绍"><a href="#xlrd、openpyxl、xlwt库介绍" class="headerlink" title="xlrd、openpyxl、xlwt库介绍"></a>xlrd、openpyxl、xlwt库介绍</h4><p>这三个库主要用来读写excel文件（.xlsx后缀格式文件），本项目只涉及读写excel文件，操作比较简单，对于更加复杂的操作还请阅读官方API。</p><h4 id="jieba库介绍"><a href="#jieba库介绍" class="headerlink" title="jieba库介绍"></a>jieba库介绍</h4><p>大家应该都听过结巴分词器，jieba这个库也是主要对地址进行分词的。但是在cpca中已经包装好了，所以本项目并没有单独用到。</p><h3 id="代码解读"><a href="#代码解读" class="headerlink" title="代码解读"></a>代码解读</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cpca</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> xlrd</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 读取excel文件，读出来的是DataFrame格式，其中keep_default_na=False将excel文件中的nan值转化为“”</span></span><br><span class="line">df = pd.read_excel(<span class="string">&#x27;fupin.xlsx&#x27;</span>,keep_default_na=<span class="literal">False</span>)</span><br><span class="line"><span class="comment"># 看一下df大小</span></span><br><span class="line"><span class="comment"># print(df.shape)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># df.values[:,4]获取的是numpy类型，然后将提取出来的numpy类型转化为list类型</span></span><br><span class="line">addresslist = df.values[:,<span class="number">4</span>].tolist()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建一个my.xlsx文件</span></span><br><span class="line">writer=pd.ExcelWriter(<span class="string">&#x27;my.xlsx&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#对每一行的地址赋值为索引i,方便后面按行写入excel文件</span></span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> addresses <span class="keyword">in</span> addresslist:</span><br><span class="line">    <span class="comment"># 如果字符串是空字符串，则对当前空字符串赋值为&quot;这是个空字符串&quot;，其实这一步也是可以省略的</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(addresses)==<span class="number">0</span>:</span><br><span class="line">        addresses = <span class="string">&quot;这是个空字符串&quot;</span></span><br><span class="line">    <span class="comment"># 对当前遍历到字符串按照&#x27;；&#x27;进行切分，转化为list格式</span></span><br><span class="line">    singl_Address_List = addresses.split(<span class="string">&#x27;；&#x27;</span>)</span><br><span class="line">    <span class="comment"># singl_Address_List的地址通过cpca进行转化DataFrame包含省市区的格式</span></span><br><span class="line">    sheng_df = cpca.transform(singl_Address_List, pos_sensitive=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 从sheng_df只提取出省这一列</span></span><br><span class="line">    xieru_df = pd.DataFrame(sheng_df.values[:,<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 对xieru_df进行转置，由一列多行转化为一行多列形式，转化为按行写入表格的形式</span></span><br><span class="line">    zhuzhi_df = pd.DataFrame(xieru_df.values.T)</span><br><span class="line"><span class="comment"># 将查不到的地址填充为&#x27;未知&#x27;标记</span></span><br><span class="line">    data_noTCS_offline = zhuzhi_df.fillna(value=<span class="string">&#x27;未知&#x27;</span>)</span><br><span class="line">    <span class="comment"># 将data_noTCS_offline写入excel，header=None表示将DataFrame格式的数据的列索引进行隐藏，startrow=i当前行写入excel时，从第i开始写</span></span><br><span class="line">    data_noTCS_offline.to_excel(writer, sheet_name=<span class="string">&#x27;Data1&#x27;</span>, startrow=i, index=<span class="literal">False</span>,header=<span class="literal">None</span>)</span><br><span class="line">    i = i+<span class="number">1</span></span><br><span class="line"><span class="comment"># 将excel进行保存，并关闭</span></span><br><span class="line">writer.save()</span><br><span class="line">writer.close()</span><br><span class="line"><span class="comment"># type函数查看数据类型    </span></span><br><span class="line"><span class="comment"># print(type(df))</span></span><br><span class="line">    </span><br><span class="line"></span><br></pre></td></tr></table></figure><p>总结：通过cpca库和pandas库，仅仅使用16行代码，便完成了整个项目。应该算是比较简便的啦，当然也告诉自己在造轮子之前先找找有没有现成的轮子。其中有几个关键点希望自己在日后可以进一步熟练掌握：</p><ul><li>处理DataFrame数据的各种操作，例如提取行列和转置</li><li>excel表格的读写操作</li></ul><h2 id="感谢和参考"><a href="#感谢和参考" class="headerlink" title="感谢和参考"></a>感谢和参考</h2><p>cpca库参考</p><p><a href="https://blog.csdn.net/weixin_34489829/article/details/113503837">https://blog.csdn.net/weixin_34489829/article/details/113503837</a></p><p><a href="https://github.com/DQinYuan/chinese_province_city_area_mapper">https://github.com/DQinYuan/chinese_province_city_area_mapper</a></p><p>python读取xlsx文件<br><a href="https://blog.51cto.com/u_15127691/4105189">https://blog.51cto.com/u_15127691/4105189</a></p><p>读取excel行列数据<br><a href="https://blog.csdn.net/weixin_45082522/article/details/106364847">https://blog.csdn.net/weixin_45082522/article/details/106364847</a><br><a href="https://zhuanlan.zhihu.com/p/409583146">https://zhuanlan.zhihu.com/p/409583146</a><br><a href="https://blog.csdn.net/QQ315806208/article/details/118547310">https://blog.csdn.net/QQ315806208/article/details/118547310</a><br><a href="https://blog.csdn.net/wl_Honest/article/details/99082977">https://blog.csdn.net/wl_Honest/article/details/99082977</a></p>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> python处理数据 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python实战项目 </tag>
            
            <tag> Python技巧学习 </tag>
            
            <tag> Python疑难杂症 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）</title>
      <link href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文Transformer的解读与总结—-《Attention-is-All-you-need》三（Why-Self-Attention部分）"><a href="#经典论文Transformer的解读与总结—-《Attention-is-All-you-need》三（Why-Self-Attention部分）" class="headerlink" title="经典论文Transformer的解读与总结—-《Attention is All you need》三（Why Self-Attention部分）"></a>经典论文Transformer的解读与总结—-《Attention is All you need》三（Why Self-Attention部分）</h1><p>作者： Aqua，苑博。</p><p>审校：苑博。</p><p>翻译：Aqua。</p><p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p><h3 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4 Why Self-Attention"></a>4 Why Self-Attention</h3><p>In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations <script type="math/tex">(x_1,x_2,\cdots,x_n)</script> to another sequence of equal length,<script type="math/tex">(z_1,z_2,\cdots,z_n)</script> with <script type="math/tex">x_i,z_i \in R^d</script>​​ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.</p><p>As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires <em>O</em>(<em>n</em>) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length <em>n</em> is smaller than the representation dimensionality <em>d</em>, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size <em>r</em> in the input sequence centered around the respective output position. This would increase the maximum path length to <em>O</em>(<em>n/r</em>). We plan to investigate this approach further in future work. </p><p>A single convolutional layer with kernel width <em>k &lt; n</em> does not connect all pairs of input and output positions. Doing so requires a stack of <em>O</em>(<em>n/k</em>) convolutional layers in the case of contiguous kernels, or <script type="math/tex">O(log_k n)</script>​ in the case of dilated convolutions, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of <em>k</em>. Separable convolutions, however, decrease the complexity considerably, to <script type="math/tex">O(knd+nd^2)</script>. Even with <em>k</em> = <em>n</em>, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.</p><p>As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.</p><p>解读：</p><p>这里将Self-Attention layers和recurrent/convolutional layers来进行比较，来说明Self-Attention的好处。假设将一个输入序列<script type="math/tex">(x_1,x_2,\cdots,x_n)</script>分别用</p><ol><li>Self-Attention Layer</li><li>Recurrent Layer</li><li>Convolutional Layer</li></ol><p>来映射到一个相同长度的序列<script type="math/tex">(z_1,z_2,\cdots,z_n)</script> ，其中<script type="math/tex">x_i,z_i \in R^d</script> 。</p><p>我们分析下面三个指标：</p><ol><li>每一层的计算复杂度</li><li>能够被并行的计算，用需要的最少的顺序操作的数量来衡量</li><li>网络中long-range dependencies的path length，在处理序列信息的任务中很重要的在于学习long-range dependencies。影响学习长距离依赖的关键点在于前向/后向信息需要传播的步长，输入和输出序列中路径越短，那么就越容易学习long-range dependencies。因此我们比较三种网络中任何输入和输出之间的最长path length</li></ol><p>结果如Table1所示。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411165302.png" alt></p><p><strong>并行计算：</strong></p><p>Self-Attention layer用一个常量级别的顺序操作，将所有的positions连接起来</p><p>Recurrent Layer需要<em>O(n)</em>个顺序操作</p><p><strong>计算复杂度分析</strong></p><p>如果序列长度<script type="math/tex">n < d</script> ，Self-Attention Layer比recurrent layers快，这对绝大部分现有模型和任务都是成立的。为了提高在序列长度很长的任务上的性能，我们对Self-Attention进行限制，只考虑输入序列中窗口为 <em>r</em>的位置上的信息，这称为Self-Attention(restricted), 这回增加maximum path length到<em>O(n/r)</em>。</p><p><strong>length path</strong></p><p>如果卷积层kernel width <script type="math/tex">k < n</script>，并不会将所有位置的输入和输出都连接起来。这样需要<script type="math/tex">O(n/k)</script>个卷积层或者<script type="math/tex">O(log_k n)</script>个dilated convolution，增加了输入输出之间的最大path length。</p><p>卷积层比循环层计算复杂度更高，是k倍。但是Separable Convolutions将见效复杂度。同时self-attention的模型可解释性更好(interpretable).</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）</title>
      <link href="/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文Transformer的解读与总结—-《Attention-is-All-you-need》二（Model-Architecture部分）"><a href="#经典论文Transformer的解读与总结—-《Attention-is-All-you-need》二（Model-Architecture部分）" class="headerlink" title="经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）"></a>经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）</h1><p>作者： Aqua，苑博。</p><p>审校：苑博。</p><p>翻译：Aqua。</p><p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p><h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3 Model Architecture"></a>3 Model Architecture</h2><p>Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations <script type="math/tex">(x_1,x_2, \cdots ,x_n)</script> to a sequence of continuous representations <script type="math/tex">z=(z_1,z_2, \cdots ,z_n)</script>. Given z, the decoder then generates an output sequence <script type="math/tex">(y_1,y_2, \cdots ,y_m)</script> of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.</p><p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p><p>解读：</p><p>这一部分其实只是一个对编码-解码器架构（encoder-decoder structure）的简单概述，编码器的主要负责将输入序列<script type="math/tex">(x_1,x_2, \cdots ,x_n)</script>映射到序列<script type="math/tex">z=(z_1,z_2, \cdots ,z_n)</script>。当获得z的时候，解码器一次生成一个元素的符号，如此一步步得到输出序列<script type="math/tex">(y_1,y_2, \cdots ,y_m)</script>。在每一步中，模型都是自回归的，在生成下一个符号时，使用之前生成的符号作为附加的输入。（隐义：这一段其实没啥精彩的地方，属于常规操作一般描述。）</p><hr><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409193149.png" alt></p><h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p><strong>Encoder:</strong> The encoder is composed of a stack of <em>N</em> = 6 identical layers. Each layer has two sub-layers.  The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.  That is, the output of each sub-layer is <script type="math/tex">LayerNorm(x + Sublayer(x))</script>, where <script type="math/tex">Sublayer(x)</script> is the function implemented by the sub-layer itself.   To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <script type="math/tex">d_{model} = 512</script>. </p><p>解读：</p><p>这一部分详细介绍了Encoder的组成，Encoder由6层堆叠而成，每一层又包括两个子层分别为Multi-head self-attention子层和Position-wise fully connected feed-forward network子层。每个子层之间又采用残差连接（Res-net）并进行归一化。为方便连接说有子层以及embedding产生的数据长度都为512。</p><p>总结：</p><p>Encoder有N=6层，每层包括两个sub-layers:</p><ol><li>第一个sub-layer是multi-head self-attention mechanism，用来计算输入的self-attention</li><li>第二个sub-layer是简单的全连接网络。</li></ol><p>在每个sub-layer我们都模拟了残差网络，每个sub-layer的输出都是</p><script type="math/tex; mode=display">LayerNorm(x + Sublayer(x))</script><p>其中Sublayer(x) 表示Sub-layer对输入x做的映射，为了确保连接，所有的sub-layers和embedding layer输出的维数都相同，维数为 <script type="math/tex">d_{model} = 512</script>。（再形象一点的举个例子，就是一个词输入词，在经过embedding层之后，变为一个长度为512的词向量<script type="math/tex">\textbf{x}=(x_1,x_2, \cdots,x_{512})</script>）</p><p>tips: 维数指的是矩阵的列</p><hr><p><strong>Decoder:</strong> The decoder is also composed of a stack of <em>N</em> = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <em>i</em> can depend only on the known outputs at positions less than <em>i</em>.</p><p>解读：</p><p>Decoder 也由6层堆叠而成，每一层又分为三个子层Mask multi-head self-attention子层、Multi-head self-attention子层（Encoder-decoder Attention）和Position-wise fully connected feed-forward network子层。每个子层之间依然采用残差连接（Res-net）并进行归一化。</p><p>总结：</p><p>Decoder也是N=6层，每层包括3个sub-layers：</p><ol><li>第一个是Masked multi-head self-attention，也是计算输入的self-attention，但是因为是生成过程，因此在时刻 i 的时候，大于 i  的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做Mask。</li><li>第二个sub-layer是对encoder的输入进行attention计算。</li><li>第三个sub-layer是全连接网络，与Encoder相同。</li></ol><p>同时Decoder中的self-attention层需要进行修改，因为只能获取到当前时刻之前的输入，因此只对时刻 t 之前的时刻输入进行attention计算，这也称为Mask操作。</p><hr><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>解读：</p><p>注意函数可以描述为将一个query和一组key-value对映射到一个输出Z，其中query、key、value和输出Z都是向量。输出Z为value的加权和，其中分配给每个value的权重由query与相应key的兼容性函数计算。(对Attention进行了一个非常简短的介绍，后文会详细给出计算公式)</p><hr><h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>We call our particular attention “Scaled Dot-Product Attention” (Figure 2). The input consists of queries and keys of dimension <script type="math/tex">d_k</script> , and values of dimension <script type="math/tex">d_v</script>. We compute the dot products of the query with all keys, divide each by <script type="math/tex">\sqrt{d_k}</script> , and apply a softmax function to obtain the weights on the values. </p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409231056.png" alt></p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <em>Q</em>.  The keys and values are also packed together into matrices <em>K</em> and <em>V</em> . We compute the matrix of outputs as:</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>The two most commonly used attention functions are additive attention, and dot-product (multi-plicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. </p><p>While for small values of  <script type="math/tex">\sqrt{d_k}</script> the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of <script type="math/tex">\sqrt{d_k}</script> . We suspect that for large values of <script type="math/tex">d_k</script>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>.</p><p>解读：</p><p>在Transformer中使用的Attention是Scaled Dot-Product Attention, 是归一化的点乘Attention，假设输入的query（q）、key（k）维度为 <script type="math/tex">d_k</script> ，value（v）维度为<script type="math/tex">d_v</script> , 那么就计算query和每个key的点乘操作，并除以 <script type="math/tex">\sqrt{d_k}</script>  ，然后应用Softmax函数计算权重。</p><script type="math/tex; mode=display">Attention(q_i,k_i,v_i) = softmax(\frac{q_ik_i^T}{\sqrt{d_k}})v_i</script><p> 在实践中，将query和keys、values分别处理为矩阵Q、K和V。矩阵Q，K和V具体的计算过程如Figure 3所示，其中<script type="math/tex">W^Q</script>，<script type="math/tex">W^K</script>和<script type="math/tex">W^V</script>为Transformer在训练过程中需要更新的权重矩阵。<script type="math/tex">X_1</script>和<script type="math/tex">X_2</script>为经过Embedding层之后输出的词向量。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410161017.png" alt></p><p>有了矩阵Q，K和V之后，计算输出矩阵Z为：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中 <script type="math/tex">Q\in R^{m×d_k}</script>、<script type="math/tex">K\in R^{m×d_k}</script>和 <script type="math/tex">V\in R^{m×d_v}</script>，输出矩阵维度为<script type="math/tex">R^{m×d_v}</script> ，这里的m表示输入的词的个数，如果为限定输入词的长度为512，则<script type="math/tex">m=d_{model}</script>，Q，K和V具体形式如Figure 4所示</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410160757.png" alt></p><p>对于其中输出Z的计算示意图如Figure 5 所示意，每个词的q会跟每一个k计算得分（相关性或者相似度），经过Softmax后就得到整个加权结果，此时每个词看的不只是它前面的序列，而是整个输入序列，通过<script type="math/tex">Attention(Q,K,V)</script>之后同一时间便可以计算出所有词的表示结果。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410163644.png" alt></p><p>Scaled Dot-Product Attention的整体示意图如Figure 2左图所示，Mask是可选的(opt.)，如果是能够获取到所有时刻的输入(K, V), 那么就不使用Mask；如果是不能获取到，那么就需要使用Mask。使用了Mask的Transformer模型也被称为Transformer Decoder，不使用Mask的Transformer模型也被称为Transformer Encoder。</p><p>为什么要除以<script type="math/tex">\sqrt{d_k}</script> ?，作者这样说到：</p><p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, <script type="math/tex">qk = \sum_{i=1}^{d_k}q_ik_i</script>, has mean 0 and variance <script type="math/tex">\sqrt{d_k}</script>.</p><p>因此，每个分量除以<script type="math/tex">\sqrt{d_k}</script>可以让点乘的方差变为1。</p><hr><h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>Instead of performing a single attention function with <script type="math/tex">d_{model}</script>-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values <em>h</em> times with different, learned linear projections to <script type="math/tex">d_k</script>, <em><script type="math/tex">d_k</script></em> and <script type="math/tex">d_v</script> dimensions, respectively.  On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding <script type="math/tex">d_{v}</script>​-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. </p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O</script><script type="math/tex; mode=display">where \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>Where the projections are parameter matrices <script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script> and  <script type="math/tex">W^O \in R^{hd_{v}×d_{model}}</script>.In this work we employ <em>h</em> = 8 parallel attention layers, or heads. For each of these we use <script type="math/tex">d_k = d_v = d_{model}/h=64</script>. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. </p><p>解读：</p><p>学习深度网络的过程，如果能够把每一层输入输出的张量大小搞懂了，那么这个网络基本上搞懂了。尤其在学习和调试Transformer代码过程中，如果能够把每一步的输入输出搞清楚，那么源码的每一步在做什么事情，也就基本能和论文对应起来了。所以说到这一步我们必须把文章前面的几个小细节回顾一下，然后才能搞懂这一部分的公式在讲什么。</p><ul><li><strong>Part one</strong>  不管是Encoder还是Decoder部分，所有的sub-layers和embedding layer输出的维数都相同，维数为 <script type="math/tex">d_{model} = 512</script>。所以<script type="math/tex">X \in R^{m×d_{model}}</script>， <script type="math/tex">Z \in R^{m×d_{model}}</script>，其中m表示输入x的个数。</li><li><strong>Part two</strong>   在3.2.1中Q、K和V大小<script type="math/tex">Q\in R^{m×d_k}</script>、<script type="math/tex">K\in R^{m×d_k}</script>和 <script type="math/tex">V\in R^{m×d_v}</script>，因为<script type="math/tex">Q=XW^Q</script>, 并且已知X和Q的大小，则<script type="math/tex">W^Q \in R^{d_{model}×d_k}</script>，同理<script type="math/tex">W^K \in R^{d_{model}×d_k}</script>，<script type="math/tex">W^V \in R^{d_{model}×d_v}</script>, 输出Z的大小通过<script type="math/tex">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script>，可得<script type="math/tex">Z\in R^{m×d_v}</script>。</li></ul><p>接下来分析<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>中的Q，K和V的大小。</p><ol><li><strong>Part one</strong>中要求<script type="math/tex">Z \in R^{m×d_{model}}</script>，2中推得<script type="math/tex">Z\in R^{m×d_v}</script>，他们都指同一个Z，结合1和2这两条，可得在3.2.1 Scaled Dot-Product Attention中得到$d<em>v$的大小必须和$$d</em>{model}<script type="math/tex">保持一致都为512，即</script>d<em>v = d</em>{model}$$。</li><li>通过<strong>Part two</strong> 还可以看到，而其中的<script type="math/tex">d_k</script>并不影响Z的维数，所以<script type="math/tex">d_k</script>的大小感觉是没有要求的。但是在论文中本部分中要求<script type="math/tex">d_k = d_v = d_{model}/h=64</script>。</li><li>对于<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>中的Q，K和V的大小而言，因为<script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script>。要想使得式中的Q，K，V满足和对应的<script type="math/tex">W_i^Q,W_i^K,W_i^V</script>相乘，则<script type="math/tex">Q\in R^{m×d_{model}}</script>、<script type="math/tex">K\in R^{m×d_{model}}</script>和 <script type="math/tex">V\in R^{m×d_{model}}</script>。</li></ol><p>对于<script type="math/tex">MultiHead(Q,K,V)</script>的公式解读：</p><ul><li>通过以上分析知道<script type="math/tex">Q\in R^{m×d_{model}}</script>，<script type="math/tex">K\in R^{m×d_{model}}</script>，<script type="math/tex">V\in R^{m×d_{model}}</script>和<script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script>，通过矩阵乘法法则可得<script type="math/tex">QW_i^Q\in R^{m×d_k}</script>、<script type="math/tex">KW_i^K\in R^{m×d_k}</script>和 <script type="math/tex">VW_i^V\in R^{m×d_v}</script>，因为<script type="math/tex">d_k = d_{model}/h=64</script>，相当于将原来<script type="math/tex">Q\in R^{m×d_{model}}</script>、<script type="math/tex">K\in R^{m×d_{model}}</script>和 <script type="math/tex">V\in R^{m×d_{model}}</script>的维数由<script type="math/tex">d_{model}=512</script>映射到了更低的维度<script type="math/tex">d_{model}/h=64</script>。</li><li>然后在采用Scaled Dot-Product Attention计算出每一个head结果，即<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>。因为在<strong>part two</strong>中，输出Z即为<script type="math/tex">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script>的计算结果，并且其中<script type="math/tex">Z\in R^{m×d_v}</script>和V的大小保持一致。以此类推<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>，因为<script type="math/tex">VW_i^V\in R^{m×d_v}</script> ，又因为<script type="math/tex">d_v = d_{model}/h=64</script>，所以<script type="math/tex">head_i \in R^{m×d_v}=R^{m×d_{model}/h}</script>。</li><li>将总共h个head的结果进行按列拼接起来，一个head是<script type="math/tex">d_v=d_{model}/h=64</script>列，h个head拼接完总共有<script type="math/tex">hd_{v}=d_{model}=512</script>列， 即<script type="math/tex">Concat(head_1,\cdots,head_h) \in R^{m × hd_v}</script>。</li><li>将合并的结果进行线性变换，因为<script type="math/tex">Concat(head_1,\cdots,head_h) \in R^{m × hd_v}</script>和，<script type="math/tex">W^O \in R^{hd_{v}×d_{model}}</script>根据矩阵乘法法则，<script type="math/tex">MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O \in R^{m×d_{model}}</script>。</li></ul><p>其更细致的流程图如Figure 6所示</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410231003.png" alt></p><hr><h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><p>The Transformer uses multi-head attention in three different ways:</p><ul><li>In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as.</li><li>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.</li><li>Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to <script type="math/tex">-\infty</script>) all values in the input of the softmax which correspond to illegal connections. See Figure 2.</li></ul><p>解读：</p><ul><li>Encoder模块的Self-Attention，在Encoder中，每层的Self-Attention的输入<script type="math/tex">Q=K=V=X</script> ，都是上一层的输出。Encoder中的每个position都能够获取到前一层的所有位置的输出。</li><li>Decoder模块的Mask Self-Attention，在Decoder中，每个position只能获取到之前position的信息，因此需要做mask，将其设置为<script type="math/tex">-\infty</script>。</li><li>Encoder-Decoder之间的Attention，其中Q来自于之前的Decoder层输出，K和V来自于encoder的输出，这样decoder的每个位置都能够获取到输入序列的所有位置信息。</li></ul><hr><h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a>3.3 Position-wise Feed-Forward Networks</h3><p>In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</p><script type="math/tex; mode=display">FFN(x)= max(0,xW_1+b_1)W_2+b_2</script><p>While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is <script type="math/tex">d_{model}=512</script>, and the inner-layer has dimensionality <script type="math/tex">d_{ff} = 2048</script>.</p><p>解读：</p><p>在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，这个层与一般的全连接不一样，对每个position的向量分别进行相同的操作，即使用相同的参数分别进行计算。总共包括两个线性变换和一个ReLU激活输出，看起来更像是两个w=1的卷积，将输入<script type="math/tex">d_{model}=512</script>的矩阵转为<script type="math/tex">d_{ff} = 2048</script>，之后再转为原始大小。除此之外其中每一层的参数都不同。</p><hr><h3 id="3-4-Embeddings-and-Softmax"><a href="#3-4-Embeddings-and-Softmax" class="headerlink" title="3.4 Embeddings and Softmax"></a>3.4 Embeddings and Softmax</h3><p>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension <script type="math/tex">d_{model}</script>. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by <script type="math/tex">\sqrt{d_{model}}</script>.</p><p>解读：</p><p>同其它模型一样，transformer的word emebdding层将输入token转换为<script type="math/tex">d_{model}</script>维词向量。在解码时，同样对输出先进行线性变换，再进行softmax归一化，得到下一个词的概率分布。</p><p>Embedding：</p><p>Transformer中使用了三处embedding：1）input embedding；2）output embedding；3）在softmax前面的Linear transformation中也使用跟embedding相同的权重。三处的embedding权重参数是共享的，这样做可以有效减少模型参数量。我们知道embedding层和Linear层的参数大小是vocab size × embedding size，其中embedding size=<script type="math/tex">d_{model}</script>，而且通常vocab size比embedding size高两个数量级，因此，共享参数能显著减少模型参数量。另外值得注意的点就是，作者把embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>，学 embedding 的时候，会把每一个向量的 Lar Norm 学的比较小。维度大的话，学到的一些权重值就会变小，但之后还需要加上 positional encoding（不会随着维度的增加而变化）。embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>使得 embedding中的权重参数 和 positional encosing中的权重参数的 scale 差不多，可以做加法。</p><p>Liner层和Softmax层：</p><p>解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。其详细输入输出的流程图如Figure 7。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411122219.png" alt></p><hr><h3 id="3-5-Positional-Encoding"><a href="#3-5-Positional-Encoding" class="headerlink" title="3.5 Positional Encoding"></a>3.5 Positional Encoding</h3><p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the. bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <script type="math/tex">d_{model}</script> as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed. In this work, we use sine and cosine functions of different frequencies:</p><script type="math/tex; mode=display">PE_{(pos,2i)}=sin(pos/10000^{2i/dmodel})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})</script><p>where <em>pos</em> is the position and <em>i</em> is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2<em>π</em> to 10000 <em>·</em> 2<em>π</em>. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset <em>k</em>, <script type="math/tex">PE_{pos+k}</script> can be represented as a linear function of <script type="math/tex">PE_{pos}</script>.</p><p>We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.</p><p>解读：</p><p>一个传统的，使用 RNN 构建的 Encoder-Decoder 模型对于输入句子中的单词是逐个读取的：读取完前一个单词，更新模型的状态，然后在此状态下再读取下一个单词。这种方式天然的包含了句子的位置前后关系。CNN模型可以有卷积的多次采样扩展得到位置信息。</p><p>Transformer模型不包括recurrence/convolution，其对整个句子的每个单词进行同时读取，因此是无法捕捉到序列顺序信息的。假如将词序打乱，或者说将K、V按行进行打乱，那么Attention之后的结果是一样的。这就与人类的语言常识相悖，通常来看，在处理时序数据的时候，如果将一句话里面的词打乱或颠倒顺序，那么语义肯定会发生变化，但是 attention 不会处理这个情况。 因此为了能够保留句子中单词和单词之间的位置关系，需要加入时序信息。也就是需要将位置也融合进入输入的句子中，即对位置进行编码。</p><p>Transformer 使用的位置编码计算公式如下：</p><script type="math/tex; mode=display">PE_{(pos,2i)}=sin(pos/10000^{2i/dmodel})</script><script type="math/tex; mode=display">PE_{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})</script><p>其中<em>pos</em>表示位置index， <em>i</em>表示dimension index。由公式来看，对于一个句子，此编码算法对于偶数位置 <em>2i</em> 和奇数位置 <em>2i+1</em> 分开进行编码。编码的结果是每个位置最终转化为<script type="math/tex">d_{model}</script>维度的向量。</p><p>虽然现在知道了Transformer的位置编码方式，学习讲究知其然知其所以然，接下来再简析一下Transformer位置编码的特点和原理。将位置编码公式拆解微观分析得：</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411151747.png" alt></p><p>并将位置编码向量按照热力图的形式进行展示得：</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411152231.png" alt></p><p>上述横坐标代表维度<em>i</em>，纵坐标代表位置。可以看到，对于句子长度最大为50的模型来说，前60维就可以区分位置了。至于后面为什么一白一蓝，别忘了，sin,cos相间。</p><p>Position Embedding本身是一个绝对位置的信息，仔细观察Transformer论文给出位置编码的公式可以发现，即只要位置小于10000，每一个位置的编码都是不同的。但在语言中，相对位置也很重要，绝对位置信息只保证了各个位置不一样，但是并不是像0,1,2这样的有明确前后关系的编码。所以需要相对位置来保证语序对应的语义。由于三角函数两角和差公式，Transformer论文给出位置编码的公式也可以很好的表示相对位置信息。</p><script type="math/tex; mode=display">sin(\alpha+\beta) = sin(\alpha)cos(\beta) + cos(\alpha)sin(\beta)</script><script type="math/tex; mode=display">cos(\alpha+\beta) = cos(\alpha)cos(\beta) - sin(\alpha)cos(\beta)</script><p>给定k,存在一个固定的与k相关的线性变换矩阵，从而由<em>pos</em>的位置编码线性变换而得到<em>pos+k</em>的位置编码。从而这个相对位置信息也可以被模型发现并利用。其证明如下：</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411153722.png" alt></p><p>序列信息代表着全局的结构，因此必须将序列的token相对或者绝对position信息利用起来。这里每个token的position embedding 向量维度也是<script type="math/tex">d_{model}=512</script>，然后将原本的embedding和position embedding加起来组成最终的embedding作为encoder/decoder的输入。positional embedding是 cos 和 sin 的一个函数，在 [-1, +1] 之间抖动的。所以embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>，使得乘积后的每个权重也是在差不多的 [-1, +1] 数值区间。相加完成在输入里面添加时序信息。不管怎么打乱输入序列的顺序，进入 layer 之后，输出那些值是不变的，最多是顺序发生了相应的变化。所以就直接把顺序信息直接加在数据值里。</p><p>:sailboat:positional embedding被加到embedding中。嵌入表示一个 <script type="math/tex">d_{model}</script>维空间的标记，在<script type="math/tex">d_{model}</script>维空间中有着相似含义的标记会离彼此更近。但是，嵌入并没有对在一句话中的词的相对位置进行编码。因此，当加上位置编码后，词将基于它们含义的相似度以及它们在句子中的位置，在<script type="math/tex">d_{model}</script> 维空间中离彼此更近。</p><p>在其他NLP论文中，大家也都看过position embedding，通常是一个训练的向量，但是position embedding只是extra features，有该信息会更好，但是没有性能也不会产生极大下降，因为RNN、CNN本身就能够捕捉到位置信息，但是在Transformer模型中，Position Embedding是位置信息的唯一来源，因此是该模型的核心成分，并非是辅助性质的特征。</p><p>也可以采用训练的position embedding，但是试验结果表明相差不大，因此论文选择了sin position embedding，因为</p><ol><li>这样可以直接计算embedding而不需要训练，减少了训练参数</li><li>这样允许模型将position embedding扩展到超过了training set中最长position的position，例如测试集中出现了更大的position，sin position embedding依然可以给出结果，但不存在训练到的embedding。</li></ol><hr><h2 id="参考及感谢"><a href="#参考及感谢" class="headerlink" title="参考及感谢"></a>参考及感谢</h2><p>Transformer Encoder部分：</p><p><a href="https://zhuanlan.zhihu.com/p/343286144">https://zhuanlan.zhihu.com/p/343286144</a></p><p>李沐讲解Transformer：</p><p><a href="https://zhuanlan.zhihu.com/p/452663865">https://zhuanlan.zhihu.com/p/452663865</a></p><p><a href="https://zhuanlan.zhihu.com/p/460607065">https://zhuanlan.zhihu.com/p/460607065</a></p><p>一个很粗糙的代码解读：</p><p><a href="https://baijiahao.baidu.com/s?id=1711290350168224964&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1711290350168224964&amp;wfr=spider&amp;for=pc</a></p><p>Transformer理解，看这篇文章读懂了embedding为什么参数共享</p><p><a href="https://blog.csdn.net/bingmeishi/article/details/105105823">https://blog.csdn.net/bingmeishi/article/details/105105823</a></p><p>Transform训练过程：</p><p><a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">https://blog.csdn.net/longxinchen_ml/article/details/86533005</a></p><p><a href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></p><p>【从 0 开始学习 Transformer】 上篇：Transformer 搭建与理解：</p><p><a href="https://zhuanlan.zhihu.com/p/97448796">https://zhuanlan.zhihu.com/p/97448796</a></p><p>【从 0 开始学习 Transformer】 下篇：Transformer 训练与评估：</p><p><a href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></p><p>自注意动图展示：</p><p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p><p>深度学习中的注意力模型（2017版）：</p><p><a href="https://zhuanlan.zhihu.com/p/37601161">https://zhuanlan.zhihu.com/p/37601161</a></p><p>Transformer学习笔记一：Positional Encoding（位置编码）</p><p><a href="https://zhuanlan.zhihu.com/p/454482273">https://zhuanlan.zhihu.com/p/454482273</a></p><p>Transformer架构：位置编码</p><p><a href="https://blog.csdn.net/Jayson13/article/details/123135888">https://blog.csdn.net/Jayson13/article/details/123135888</a></p><p>深入理解transformer中的位置编码</p><p><a href="https://www.csdn.net/tags/OtDacg3sNDA1NjUtYmxvZwO0O0OO0O0O.html">https://www.csdn.net/tags/OtDacg3sNDA1NjUtYmxvZwO0O0OO0O0O.html</a></p><p><a href="https://blog.csdn.net/qq_43391414/article/details/121061766">https://blog.csdn.net/qq_43391414/article/details/121061766</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）</title>
      <link href="/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文的解读与厚黑—-《Attention-is-All-you-need》一（Introduction和Background部分）"><a href="#经典论文的解读与厚黑—-《Attention-is-All-you-need》一（Introduction和Background部分）" class="headerlink" title="经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）"></a>经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）</h1><p>作者： Aqua，苑博。</p><p>审校：苑博。</p><p>翻译：Aqua。</p><p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p><p>《Attention is All you need》你真的读懂了吗？不管读没读过，这篇论文都值得多读几遍。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Recurrent neural networks, long short-term memory and gated recurrent  neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.</p><p>解读与厚黑：</p><p>目前来看，循环神经网络（RNN），如长短期记忆和门控神经网络模型（LSTM），在语言模型和机器翻译领域一直处于主导地位。可惜的是现在的大部分研究和努力只是游离在RNN和编码器-解码器架构的边界而已，意义不大。（隐义：现在循环神经网络很火，虽然大家都在凑这个热闹，但都是些小打小闹，没啥突破性进展。是时候换换天了。为Transformer做铺垫）</p><hr><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <script type="math/tex">h_t</script> , as a function of the previous hidden state <script type="math/tex">h_{t-1}</script>​​​ and the input for position <em>t</em>. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p><p>解读与厚黑：</p><p>简单说说RNN存在的问题， t时刻的计算<script type="math/tex">h_t</script>依赖于 t-1 时刻的计算结果<script type="math/tex">h_{t-1}</script>，这种由前往后顺序计算的形式，会存在长距离的依赖问题，这样不仅限制了模型的并行能力。还使得当前词语获取到的上下文信息受限。即使现在的很多研究想通过factorization tricks 和conditional computation来提升模型的计算效率和性能表现，但是依然没有改变RNN天生顺序约束的缺陷。（隐义：RNN的这种顺序的结构设计就有问题，你们这些跟随者即使做再多RNN局部优化的尝试，也多是隔靴挠痒，意义不大。只有从根本上改变，才能实现真正的突破，比如注意力机制）</p><hr><p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.</p><p>解读与厚黑：</p><p>众所周知，在翻译过程中，不同的英文对中文的依赖程度不同。还有文本在表征语义时，当前词需要获取的信息可能来自距离很远的另一个词。 注意力机制（Attention mechanisms）通过将序列中任意两个位置之间的距离缩小为一个常量 ，从而解决长距离依赖的问题。这种对依赖关系进行建模的方式，不仅可以很好的适应了我们人脑的认知，还能够实现并行。在大多数情况下，注意机制与循环网络一起使用往往会产生很好的效果。（隐义：你看注意力机制就不一样了吧，把你最根本的问题解决了，并且我还发现Attention搭配上RNN，效果倍棒。方向给你们了，自己看着办！）</p><hr><p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p><p>解读与厚黑：</p><p>Transformer的核心就是使用Attention来绘制输入和输出之间的全局依赖关系，除此之外，还可以使用并行训练的方式大大缩短训练时间。（隐义：咱们这个Transformer内部用的可是Attention，训练快效果好，洋气又时髦，了解一下啊大哥）</p><hr><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet  and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.  In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p><p>解读与厚黑：</p><p>RNN的最大问题在于顺序计算，虽然通过卷积神经网络的方式可以减少顺序计算，实现并行计算。但是模型在学习任意两个位置之间的依赖关系时，所需的操作数随着距离的增长而增长。这就使得模型学习遥远位置之间的依赖关系要更加困难。而在Transformer中，学习任意两个位置之间的依赖关系所需的操作数被减少为恒定次数。所以Transformer要相比于卷积的形式就来的更加有效率。（隐义：虽然你卷积可以并行，但是你有长距离难计算的问题啊，我的Transformer就不同了，不仅具有你并行的好处，还能轻松计算任意位置间的依赖关系。卷积你不太行啊！）</p><hr><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.</p><p>解读与厚黑：</p><p>自注意机制作为注意机制中的一种，将单个序列的不同位置联系起来，以计算序列的表示。同时自注意机制已成功地应用于各种任务中，包括阅读理解、抽象摘要和文本隐含等。（隐义：自注意力机制对各种NLP任务的适用性都挺好，东西不错，Transformer也用用。）</p><hr><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks</p><p>直接厚黑：</p><p>End-to-end memory networks也是个好东西，借鉴一下。</p><hr><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models.</p><p>解读与厚黑：</p><p>Transformer 是第一个完全依赖于Self-attention来计算其输入和输出表示的transduction model，而不使用序列对齐的RNN或卷积神经网络。（隐义：Transformer结合了Self-attention和End-to-end memory networks的好处，可以说是开创性尝试，下面正式开吹。）</p><hr><h2 id="参考及感谢"><a href="#参考及感谢" class="headerlink" title="参考及感谢"></a>参考及感谢</h2><p>LSTM 已死，事实真是这样吗？</p><p><a href="https://view.inews.qq.com/a/20220325A03LNX00">https://view.inews.qq.com/a/20220325A03LNX00</a></p><p>Attention is all you need 详解Transformer</p><p><a href="https://www.cnblogs.com/zhanghaiyan/p/11079504.html">https://www.cnblogs.com/zhanghaiyan/p/11079504.html</a></p><p>Attention is all you need博客</p><p><a href="https://blog.csdn.net/sinat_33741547/article/details/85884052">https://blog.csdn.net/sinat_33741547/article/details/85884052</a></p><p>论文解读:Attention is All you need</p><p><a href="https://zhuanlan.zhihu.com/p/46990010">https://zhuanlan.zhihu.com/p/46990010</a></p><p>长依赖问题：</p><p><a href="https://zhuanlan.zhihu.com/p/69704935">https://zhuanlan.zhihu.com/p/69704935</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试算法100问---两数相加</title>
      <link href="/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
      <url>/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="面试算法100问—两数相加"><a href="#面试算法100问—两数相加" class="headerlink" title="面试算法100问—两数相加"></a>面试算法100问—两数相加</h1><p>面试官：Samira</p><p>应聘者：Aqua</p><p>Samira：”接下来我们做道题吧！“</p><p>Aqua（紧张）：”可以的。“</p><p>Samira在视频面试的做题面板上开始不紧不慢的敲击键盘。Aqua本以为Samira会敲出一道完整且表述清晰的题目，可惜他多想了，Samira只是简单给了一个数组nums&#x3D;[2,7,11,15]和target&#x3D;9，然后开始口述。</p><p>Samira：”这个数组nums和target可以看到吧，你现在要在nums中找到和为target的两个整数，然后返回这两个整数的索引。“</p><p>Aqua直接满脸问号，第一次面本来就紧张，还是口述题目，口述也就算了，关键面试官还带口音。直接当场麻掉。</p><p>Samira（很轻松）：”你不用那么紧张，如果有思路可以先聊聊看。“</p><p>Aqua思考了一会，好像回忆起来了，这个听着有点像那个两数相加。</p><p>Aqua（保持假笑）：“请问还有其他要求吗？比如每个nums只有一个答案，同时nums中同一个元素在答案中不能重复出现。”</p><p>Samira：“很好，要求如你所说，并且你可以按任意顺序返回答案。”</p><p>Aqua一边在心里抱怨，为什么不讲清楚，面试官都是玩猜的吗？一边在心里亲切的问候着Samira的家人。然后又继续想了一分钟</p><p>下边附加上LeetCode原题描述：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。</span><br><span class="line">你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</span><br><span class="line">你可以按任意顺序返回答案。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line">输入：nums = [<span class="number">2</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">15</span>], target = <span class="number">9</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">解释：因为 nums[<span class="number">0</span>] + nums[<span class="number">1</span>] == <span class="number">9</span> ，返回 [<span class="number">0</span>, <span class="number">1</span>] 。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">2</span>：</span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>], target = <span class="number">6</span></span><br><span class="line">输出：[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">示例 <span class="number">3</span>：</span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">3</span>], target = <span class="number">6</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406203823.png"></p><p>Aqua：“我需要从nums中找到，唯一的一对索引答案[pos1,pos2]，假设我是按照顺序输出的，即pos1在pos2之前，如果如果能够找到pos2，那么只需要在pos2位置之前的元素中寻找pos1即可。这样我便可以遍历nums，假设当前遍历到nums[i]的索引 i （0&lt;&#x3D;i&lt;n）是满足题意的pos2，那么只需看在索引 i 之前是否存在target-nums[i]，若存在，这元素target-nums[i]的索引 j 即为pos1，索引 i 即为pos2。现在的问题是怎么找target-nums[i]，难道从0到 i 再遍历一遍nums，这样去寻找target-nums[i]这样的话时间复杂度就是O(n)。而寻找pos2的过程的时间复杂度为O(n)，这样总的时间复杂度就是O(n*n)。”</p><p>Simara：“可以再降低一下时间复杂度吗？”</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406212608.png"></p><p>Aqua：“那就牺牲空间来减少时间吧，如果我能够在每次遍历到nums[i]的时候都能够把nums[i]之前的元素存储在HashMap中，把nums[j]当做key，把索引j作为value，0&lt;&#x3D;j&lt;i。这样在HashMap中寻找target-nums[i]也就是寻找pos1的时间复杂度为O(1)，寻找pos2的过程中需要遍历nums时间复杂度为O(n)，所以总的时间复杂度为O(n)。同时，因为这个过程用到了HashMap，所以空间复杂度为O(n)。“</p><p>Simara：“你是想把所有的nums中元素和其对应的索引一次性全部放到HashMap中，还是在遍历过程中进行动态扩充HashMap？”</p><p>Aqua：“进行动态扩充吧，如果我在遍历nums的过程其实就是寻找pos2的过程，如果遍历到的nums[i]的索引 i 不是答案中pos2，也就是在nums[i]之前不存在target-nums[i]，那么说明pos2还在当前 i 的后面，也就是说这个nums[i] 和索引 i 是需要加入到HashMap中的，那么直接加入HashMap。”</p><p>Simara：“那按照你的思路实现一下代码吧！”</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Solution</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">    <span class="keyword">if</span>(nums == <span class="literal">null</span> || nums.length == <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">0</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span>[] pos = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">    HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Integer,Integer&gt;();</span><br><span class="line">    <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> nums.length;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(map.containsKey(target-nums[i]))&#123;</span><br><span class="line">    pos[<span class="number">0</span>] = map.get(target-nums[i]);</span><br><span class="line">    pos[<span class="number">1</span>] = i;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    map.put(nums[i], i);</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">return</span> pos;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 面试算法100问 </category>
          
          <category> HashMap </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试算法题 </tag>
            
            <tag> 简单算法题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>启程算法之旅</title>
      <link href="/2022/03/26/%E5%90%AF%E7%A8%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E6%97%85/"/>
      <url>/2022/03/26/%E5%90%AF%E7%A8%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E6%97%85/</url>
      
        <content type="html"><![CDATA[<h1 id="博客说明"><a href="#博客说明" class="headerlink" title="博客说明"></a>博客说明</h1><p>自己一直有记笔记的习惯，感觉这样可以很好减少重复学习的时间。虽然现在CSDN和知乎都可以通过博客的方式来记录学习内容，但是总感觉他们那个书写界面不太适合自己。所以平常都是通过Typora记录在本地，随着记录的内容不断增多，感觉总是在本地和手机之间传来传去有点不太方便。所以就想做个个人博客吧，这样既能继续保留Typora写文档的习惯，还能把学的东西系统化的展示出来，同时也能看的方便。这么一琢磨，确实是时候做一做了。</p><p>经过两天的学习，终于把这个博客搞完了。除了界面的美化不能和专业前端相比之外，现有的功能已经基本满足个人的使用需求。根据目前的情况，以后也会不定期的更新博客，后续内容主要分两方面：第一方面主要是一些常见的机器学习算法的应用和理论（LR，SVM，随机森林，决策树，GBDT，XGboost，LightGBM，贝叶斯网络，HMM，CRF和聚类等）。对于深度学习这一块，因为自己主要搞NLP方向，所以会集中讲一些深度学习框架应用（CNN，RNN，Word2Vector，LSTM，Transformer，BERT，RoBert，XLBert和XLNet等）。最后是希望自己可以保证精产还能多产。当然最重要的是希望通过写博客这样一种方式，可以让自己的学习过程能更扎实一些。也让自己对技术的掌握更熟练一些。毕竟技术的尽头还是技术。</p><h2 id="Hexo个人常用命令"><a href="#Hexo个人常用命令" class="headerlink" title="Hexo个人常用命令"></a>Hexo个人常用命令</h2><p>为了减少重复搜索，还是记下来比较好</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s <span class="comment">#启动本地静态页面</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g <span class="comment">#生成静态文件</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d <span class="comment">#部署到远程站点</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> Hexo使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
            <tag> Hexo框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/25/hello-world/"/>
      <url>/2022/03/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><p>新增测试</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
            <tag> 个人建设 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
