<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>经典论文Transformer的解读与总结---《Attention is All you need》三（Why Self-Attention部分）</title>
      <link href="/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/11/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87Transformer%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E6%80%BB%E7%BB%93---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%E4%B8%89%EF%BC%88Why%20Self-Attention%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文Transformer的解读与总结—-《Attention-is-All-you-need》三（Why-Self-Attention部分）"><a href="#经典论文Transformer的解读与总结—-《Attention-is-All-you-need》三（Why-Self-Attention部分）" class="headerlink" title="经典论文Transformer的解读与总结—-《Attention is All you need》三（Why Self-Attention部分）"></a>经典论文Transformer的解读与总结—-《Attention is All you need》三（Why Self-Attention部分）</h1><p>作者： Aqua，苑博。</p><p>审校：苑博。</p><p>翻译：Aqua。</p><p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p><h3 id="4-Why-Self-Attention"><a href="#4-Why-Self-Attention" class="headerlink" title="4 Why Self-Attention"></a>4 Why Self-Attention</h3><p>In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations <script type="math/tex">(x_1,x_2,\cdots,x_n)</script> to another sequence of equal length,<script type="math/tex">(z_1,z_2,\cdots,z_n)</script> with <script type="math/tex">x_i,z_i \in R^d</script>​​ , such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types.</p><p>As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires <em>O</em>(<em>n</em>) sequential operations. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length <em>n</em> is smaller than the representation dimensionality <em>d</em>, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece and byte-pair representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size <em>r</em> in the input sequence centered around the respective output position. This would increase the maximum path length to <em>O</em>(<em>n/r</em>). We plan to investigate this approach further in future work. </p><p>A single convolutional layer with kernel width <em>k &lt; n</em> does not connect all pairs of input and output positions. Doing so requires a stack of <em>O</em>(<em>n/k</em>) convolutional layers in the case of contiguous kernels, or <script type="math/tex">O(log_k n)</script>​ in the case of dilated convolutions, increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of <em>k</em>. Separable convolutions, however, decrease the complexity considerably, to <script type="math/tex">O(knd+nd^2)</script>. Even with <em>k</em> = <em>n</em>, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model.</p><p>As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.</p><p>解读：</p><p>这里将Self-Attention layers和recurrent/convolutional layers来进行比较，来说明Self-Attention的好处。假设将一个输入序列<script type="math/tex">(x_1,x_2,\cdots,x_n)</script>分别用</p><ol><li>Self-Attention Layer</li><li>Recurrent Layer</li><li>Convolutional Layer</li></ol><p>来映射到一个相同长度的序列<script type="math/tex">(z_1,z_2,\cdots,z_n)</script> ，其中<script type="math/tex">x_i,z_i \in R^d</script> 。</p><p>我们分析下面三个指标：</p><ol><li>每一层的计算复杂度</li><li>能够被并行的计算，用需要的最少的顺序操作的数量来衡量</li><li>网络中long-range dependencies的path length，在处理序列信息的任务中很重要的在于学习long-range dependencies。影响学习长距离依赖的关键点在于前向/后向信息需要传播的步长，输入和输出序列中路径越短，那么就越容易学习long-range dependencies。因此我们比较三种网络中任何输入和输出之间的最长path length</li></ol><p>结果如Table1所示。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411165302.png" alt></p><p><strong>并行计算：</strong></p><p>Self-Attention layer用一个常量级别的顺序操作，将所有的positions连接起来</p><p>Recurrent Layer需要<em>O(n)</em>个顺序操作</p><p><strong>计算复杂度分析</strong></p><p>如果序列长度<script type="math/tex">n < d</script> ，Self-Attention Layer比recurrent layers快，这对绝大部分现有模型和任务都是成立的。为了提高在序列长度很长的任务上的性能，我们对Self-Attention进行限制，只考虑输入序列中窗口为 <em>r</em>的位置上的信息，这称为Self-Attention(restricted), 这回增加maximum path length到<em>O(n/r)</em>。</p><p><strong>length path</strong></p><p>如果卷积层kernel width <script type="math/tex">k < n</script>，并不会将所有位置的输入和输出都连接起来。这样需要<script type="math/tex">O(n/k)</script>个卷积层或者<script type="math/tex">O(log_k n)</script>个dilated convolution，增加了输入输出之间的最大path length。</p><p>卷积层比循环层计算复杂度更高，是k倍。但是Separable Convolutions将见效复杂度。同时self-attention的模型可解释性更好(interpretable).</p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典论文Transformer的解读与总结---《Attention is All you need》二（Model Architecture部分）</title>
      <link href="/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/09/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Model%20Architecture%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文Transformer的解读与总结—-《Attention-is-All-you-need》二（Model-Architecture部分）"><a href="#经典论文Transformer的解读与总结—-《Attention-is-All-you-need》二（Model-Architecture部分）" class="headerlink" title="经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）"></a>经典论文Transformer的解读与总结—-《Attention is All you need》二（Model Architecture部分）</h1><p>作者： Aqua，苑博。</p><p>审校：苑博。</p><p>翻译：Aqua。</p><p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p><h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3 Model Architecture"></a>3 Model Architecture</h2><p>Most competitive neural sequence transduction models have an encoder-decoder structure. Here, the encoder maps an input sequence of symbol representations <script type="math/tex">(x_1,x_2, \cdots ,x_n)</script> to a sequence of continuous representations <script type="math/tex">z=(z_1,z_2, \cdots ,z_n)</script>. Given z, the decoder then generates an output sequence <script type="math/tex">(y_1,y_2, \cdots ,y_m)</script> of symbols one element at a time. At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next.</p><p>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.</p><p>解读：</p><p>这一部分其实只是一个对编码-解码器架构（encoder-decoder structure）的简单概述，编码器的主要负责将输入序列<script type="math/tex">(x_1,x_2, \cdots ,x_n)</script>映射到序列<script type="math/tex">z=(z_1,z_2, \cdots ,z_n)</script>。当获得z的时候，解码器一次生成一个元素的符号，如此一步步得到输出序列<script type="math/tex">(y_1,y_2, \cdots ,y_m)</script>。在每一步中，模型都是自回归的，在生成下一个符号时，使用之前生成的符号作为附加的输入。（隐义：这一段其实没啥精彩的地方，属于常规操作一般描述。）</p><hr><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409193149.png" alt></p><h3 id="3-1-Encoder-and-Decoder-Stacks"><a href="#3-1-Encoder-and-Decoder-Stacks" class="headerlink" title="3.1 Encoder and Decoder Stacks"></a>3.1 Encoder and Decoder Stacks</h3><p><strong>Encoder:</strong> The encoder is composed of a stack of <em>N</em> = 6 identical layers. Each layer has two sub-layers.  The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.  That is, the output of each sub-layer is <script type="math/tex">LayerNorm(x + Sublayer(x))</script>, where <script type="math/tex">Sublayer(x)</script> is the function implemented by the sub-layer itself.   To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <script type="math/tex">d_{model} = 512</script>. </p><p>解读：</p><p>这一部分详细介绍了Encoder的组成，Encoder由6层堆叠而成，每一层又包括两个子层分别为Multi-head self-attention子层和Position-wise fully connected feed-forward network子层。每个子层之间又采用残差连接（Res-net）并进行归一化。为方便连接说有子层以及embedding产生的数据长度都为512。</p><p>总结：</p><p>Encoder有N=6层，每层包括两个sub-layers:</p><ol><li>第一个sub-layer是multi-head self-attention mechanism，用来计算输入的self-attention</li><li>第二个sub-layer是简单的全连接网络。</li></ol><p>在每个sub-layer我们都模拟了残差网络，每个sub-layer的输出都是</p><script type="math/tex; mode=display">LayerNorm(x + Sublayer(x))</script><p>其中Sublayer(x) 表示Sub-layer对输入x做的映射，为了确保连接，所有的sub-layers和embedding layer输出的维数都相同，维数为 <script type="math/tex">d_{model} = 512</script>。（再形象一点的举个例子，就是一个词输入词，在经过embedding层之后，变为一个长度为512的词向量<script type="math/tex">\textbf{x}=(x_1,x_2, \cdots,x_{512})</script>）</p><p>:tada:维数指的是矩阵的列</p><hr><p><strong>Decoder:</strong> The decoder is also composed of a stack of <em>N</em> = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position <em>i</em> can depend only on the known outputs at positions less than <em>i</em>.</p><p>解读：</p><p>Decoder 也由6层堆叠而成，每一层又分为三个子层Mask multi-head self-attention子层、Multi-head self-attention子层（Encoder-decoder Attention）和Position-wise fully connected feed-forward network子层。每个子层之间依然采用残差连接（Res-net）并进行归一化。</p><p>总结：</p><p>Decoder也是N=6层，每层包括3个sub-layers：</p><ol><li>第一个是Masked multi-head self-attention，也是计算输入的self-attention，但是因为是生成过程，因此在时刻 i 的时候，大于 i  的时刻都没有结果，只有小于 i 的时刻有结果，因此需要做Mask。</li><li>第二个sub-layer是对encoder的输入进行attention计算。</li><li>第三个sub-layer是全连接网络，与Encoder相同。</li></ol><p>同时Decoder中的self-attention层需要进行修改，因为只能获取到当前时刻之前的输入，因此只对时刻 t 之前的时刻输入进行attention计算，这也称为Mask操作。</p><hr><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><p>An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.</p><p>解读：</p><p>注意函数可以描述为将一个query和一组key-value对映射到一个输出Z，其中query、key、value和输出Z都是向量。输出Z为value的加权和，其中分配给每个value的权重由query与相应key的兼容性函数计算。(对Attention进行了一个非常简短的介绍，后文会详细给出计算公式)</p><hr><h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p>We call our particular attention “Scaled Dot-Product Attention” (Figure 2). The input consists of queries and keys of dimension <script type="math/tex">d_k</script> , and values of dimension <script type="math/tex">d_v</script>. We compute the dot products of the query with all keys, divide each by <script type="math/tex">\sqrt{d_k}</script> , and apply a softmax function to obtain the weights on the values. </p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220409231056.png" alt></p><p>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix <em>Q</em>.  The keys and values are also packed together into matrices <em>K</em> and <em>V</em> . We compute the matrix of outputs as:</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>The two most commonly used attention functions are additive attention, and dot-product (multi-plicative) attention.  Dot-product attention is identical to our algorithm, except for the scaling factor of <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. </p><p>While for small values of  <script type="math/tex">\sqrt{d_k}</script> the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of <script type="math/tex">\sqrt{d_k}</script> . We suspect that for large values of <script type="math/tex">d_k</script>, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients. To counteract this effect, we scale the dot products by <script type="math/tex">\frac{1}{\sqrt{d_k}}</script>.</p><p>解读：</p><p>在Transformer中使用的Attention是Scaled Dot-Product Attention, 是归一化的点乘Attention，假设输入的query（q）、key（k）维度为 <script type="math/tex">d_k</script> ，value（v）维度为<script type="math/tex">d_v</script> , 那么就计算query和每个key的点乘操作，并除以 <script type="math/tex">\sqrt{d_k}</script>  ，然后应用Softmax函数计算权重。</p><script type="math/tex; mode=display">Attention(q_i,k_i,v_i) = softmax(\frac{q_ik_i^T}{\sqrt{d_k}})v_i</script><p> 在实践中，将query和keys、values分别处理为矩阵Q、K和V。矩阵Q，K和V具体的计算过程如Figure 3所示，其中<script type="math/tex">W^Q</script>，<script type="math/tex">W^K</script>和<script type="math/tex">W^V</script>为Transformer在训练过程中需要更新的权重矩阵。<script type="math/tex">X_1</script>和<script type="math/tex">X_2</script>为经过Embedding层之后输出的词向量。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410161017.png" alt></p><p>有了矩阵Q，K和V之后，计算输出矩阵Z为：</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>其中 <script type="math/tex">Q\in R^{m×d_k}</script>、<script type="math/tex">K\in R^{m×d_k}</script>和 <script type="math/tex">V\in R^{m×d_v}</script>，输出矩阵维度为<script type="math/tex">R^{m×d_v}</script> ，这里的m表示输入的词的个数，如果为限定输入词的长度为512，则<script type="math/tex">m=d_{model}</script>，Q，K和V具体形式如Figure 4所示</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410160757.png" alt></p><p>对于其中输出Z的计算示意图如Figure 5 所示意，每个词的q会跟每一个k计算得分（相关性或者相似度），经过Softmax后就得到整个加权结果，此时每个词看的不只是它前面的序列，而是整个输入序列，通过<script type="math/tex">Attention(Q,K,V)</script>之后同一时间便可以计算出所有词的表示结果。</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410163644.png" alt></p><p>Scaled Dot-Product Attention的整体示意图如Figure 2左图所示，Mask是可选的(opt.)，如果是能够获取到所有时刻的输入(K, V), 那么就不使用Mask；如果是不能获取到，那么就需要使用Mask。使用了Mask的Transformer模型也被称为Transformer Decoder，不使用Mask的Transformer模型也被称为Transformer Encoder。</p><p>为什么要除以<script type="math/tex">\sqrt{d_k}</script> ?，作者这样说到：</p><p>To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, <script type="math/tex">qk = \sum_{i=1}^{d_k}q_ik_i</script>, has mean 0 and variance <script type="math/tex">\sqrt{d_k}</script>.</p><p>因此，每个分量除以<script type="math/tex">\sqrt{d_k}</script>可以让点乘的方差变为1。</p><hr><h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><p>Instead of performing a single attention function with <script type="math/tex">d_{model}</script>-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values <em>h</em> times with different, learned linear projections to <script type="math/tex">d_k</script>, <em><script type="math/tex">d_k</script></em> and <script type="math/tex">d_v</script> dimensions, respectively.  On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding <script type="math/tex">d_{v}</script>​-dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. </p><p>Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.</p><script type="math/tex; mode=display">MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O\\where \  head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>Where the projections are parameter matrices <script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script> and  <script type="math/tex">W^O \in R^{hd_{v}×d_{model}}</script>.</p><p>In this work we employ <em>h</em> = 8 parallel attention layers, or heads. For each of these we use <script type="math/tex">d_k = d_v = d_{model}/h=64</script>. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. </p><p>解读：</p><p>学习深度网络的过程，如果能够把每一层输入输出的张量大小搞懂了，那么这个网络基本上搞懂了。尤其在学习和调试Transformer代码过程中，如果能够把每一步的输入输出搞清楚，那么源码的每一步在做什么事情，也就基本能和论文对应起来了。所以说到这一步我们必须把文章前面的几个小细节回顾一下，然后才能搞懂这一部分的公式在讲什么。</p><ul><li><strong>Part one</strong>  不管是Encoder还是Decoder部分，所有的sub-layers和embedding layer输出的维数都相同，维数为 <script type="math/tex">d_{model} = 512</script>。所以<script type="math/tex">X \in R^{m×d_{model}}</script>， <script type="math/tex">Z \in R^{m×d_{model}}</script>，其中m表示输入x的个数。</li><li><strong>Part two</strong>   在3.2.1中Q、K和V大小<script type="math/tex">Q\in R^{m×d_k}</script>、<script type="math/tex">K\in R^{m×d_k}</script>和 <script type="math/tex">V\in R^{m×d_v}</script>，因为<script type="math/tex">Q=XW^Q</script>, 并且已知X和Q的大小，则<script type="math/tex">W^Q \in R^{d_{model}×d_k}</script>，同理<script type="math/tex">W^K \in R^{d_{model}×d_k}</script>，<script type="math/tex">W^V \in R^{d_{model}×d_v}</script>, 输出Z的大小通过<script type="math/tex">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script>，可得<script type="math/tex">Z\in R^{m×d_v}</script>。</li></ul><p>接下来分析<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>中的Q，K和V的大小。</p><ol><li><strong>Part one</strong>中要求<script type="math/tex">Z \in R^{m×d_{model}}</script>，2中推得<script type="math/tex">Z\in R^{m×d_v}</script>，他们都指同一个Z，结合1和2这两条，可得在3.2.1 Scaled Dot-Product Attention中得到$d<em>v$的大小必须和$$d</em>{model}<script type="math/tex">保持一致都为512，即</script>d<em>v = d</em>{model}$$。</li><li>通过<strong>Part two</strong> 还可以看到，而其中的<script type="math/tex">d_k</script>并不影响Z的维数，所以<script type="math/tex">d_k</script>的大小感觉是没有要求的。但是在论文中本部分中要求<script type="math/tex">d_k = d_v = d_{model}/h=64</script>。</li><li>对于<script type="math/tex">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script>中的Q，K和V的大小而言，因为<script type="math/tex">W_i^Q \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^K \in R^{d_{model}×d_k}</script>, <script type="math/tex">W_i^V \in R^{d_{model}×d_v}</script>。要想使得式中的Q，K，V满足和对应的<script type="math/tex">W_i^Q,W_i^K,W_i^V</script>相乘，则<script type="math/tex">Q\in R^{m×d_{model}}</script>、<script type="math/tex">K\in R^{m×d_{model}}</script>和 <script type="math/tex">V\in R^{m×d_{model}}</script>。</li></ol><script type="math/tex; mode=display">MultiHead(Q,K,V)$$的公式解读：- 通过以上分析知道$$Q\in R^{m×d_{model}}$$，$$K\in R^{m×d_{model}}$$，$$V\in R^{m×d_{model}}$$和$$W_i^Q \in R^{d_{model}×d_k}$$, $$W_i^K \in R^{d_{model}×d_k}$$, $$W_i^V \in R^{d_{model}×d_v}$$，通过矩阵乘法法则可得$$QW_i^Q\in R^{m×d_k}$$、$$KW_i^K\in R^{m×d_k}$$和 $$VW_i^V\in R^{m×d_v}$$，因为$$d_k = d_{model}/h=64$$，相当于将原来$$Q\in R^{m×d_{model}}$$、$$K\in R^{m×d_{model}}$$和 $$V\in R^{m×d_{model}}$$的维数由$$d_{model}=512$$映射到了更低的维度$$d_{model}/h=64$$。- 然后在采用Scaled Dot-Product Attention计算出每一个head结果，即$$head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$。因为在**part two**中，输出Z即为$$Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$的计算结果，并且其中$$Z\in R^{m×d_v}$$和V的大小保持一致。以此类推$$head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)$$，因为$$VW_i^V\in R^{m×d_v}$$ ，又因为$$d_v = d_{model}/h=64$$，所以$$head_i \in R^{m×d_v}=R^{m×d_{model}/h}$$。- 将总共h个head的结果进行按列拼接起来，一个head是$$d_v=d_{model}/h=64$$列，h个head拼接完总共有$$hd_{v}=d_{model}=512$$列， 即$$Concat(head_1,\cdots,head_h) \in R^{m × hd_v}$$。- 将合并的结果进行线性变换，因为$$Concat(head_1,\cdots,head_h) \in R^{m × hd_v}$$和，$$W^O \in R^{hd_{v}×d_{model}}$$根据矩阵乘法法则，$$MultiHead(Q,K,V)=Concat(head_1,\cdots,head_h)W^O \in R^{m×d_{model}}$$。其更细致的流程图如Figure 6所示![](https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220410231003.png)------#### 3.2.3 Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:- In "encoder-decoder attention" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as.- The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.- Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to $$-\infty$$) all values in the input of the softmax which correspond to illegal connections. See Figure 2.解读：- Encoder模块的Self-Attention，在Encoder中，每层的Self-Attention的输入$$Q=K=V=X$$ ，都是上一层的输出。Encoder中的每个position都能够获取到前一层的所有位置的输出。- Decoder模块的Mask Self-Attention，在Decoder中，每个position只能获取到之前position的信息，因此需要做mask，将其设置为$$-\infty$$。- Encoder-Decoder之间的Attention，其中Q来自于之前的Decoder层输出，K和V来自于encoder的输出，这样decoder的每个位置都能够获取到输入序列的所有位置信息。------### 3.3 Position-wise Feed-Forward NetworksIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.</script><p>FFN(x)= max(0,xW_1+b_1)W_2+b_2</p><script type="math/tex; mode=display">While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is $$d_{model}=512$$, and the inner-layer has dimensionality $$d_{ff} = 2048$$.解读：在进行了Attention操作之后，encoder和decoder中的每一层都包含了一个全连接前向网络，这个层与一般的全连接不一样，对每个position的向量分别进行相同的操作，即使用相同的参数分别进行计算。总共包括两个线性变换和一个ReLU激活输出，看起来更像是两个w=1的卷积，将输入$$d_{model}=512$$的矩阵转为$$d_{ff} = 2048$$，之后再转为原始大小。除此之外其中每一层的参数都不同。------### 3.4 Embeddings and SoftmaxSimilarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $$d_{model}$$. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [24]. In the embedding layers, we multiply those weights by $$\sqrt{d_{model}}$$.解读：同其它模型一样，transformer的word emebdding层将输入token转换为$$d_{model}$$维词向量。在解码时，同样对输出先进行线性变换，再进行softmax归一化，得到下一个词的概率分布。Embedding：Transformer中使用了三处embedding：1）input embedding；2）output embedding；3）在softmax前面的Linear transformation中也使用跟embedding相同的权重。三处的embedding权重参数是共享的，这样做可以有效减少模型参数量。我们知道embedding层和Linear层的参数大小是vocab_size × embedding_size，其中$$embedding\_size=d_{model}$$，而且通常vocab_size比embedding_size高两个数量级，因此，共享参数能显著减少模型参数量。另外值得注意的点就是，作者把embedding中的权重参数都乘上了$$\sqrt{d_{model}}$$，学 embedding 的时候，会把每一个向量的 Lar Norm 学的比较小。维度大的话，学到的一些权重值就会变小，但之后还需要加上 positional encoding（不会随着维度的增加而变化）。embedding中的权重参数都乘上了$$\sqrt{d_{model}}$$使得 embedding中的权重参数 和 positional encosing中的权重参数的 scale 差不多，可以做加法。Liner层和Softmax层：解码组件最后会输出一个实数向量。我们如何把浮点数变成一个单词？这便是线性变换层要做的工作，它之后就是Softmax层。线性变换层是一个简单的全连接神经网络，它可以把解码组件产生的向量投射到一个比它大得多的、被称作对数几率（logits）的向量里。不妨假设我们的模型从训练集中学习一万个不同的英语单词（我们模型的“输出词表”）。因此对数几率向量为一万个单元格长度的向量——每个单元格对应某一个单词的分数。接下来的Softmax 层便会把那些分数变成概率（都为正数、上限1.0）。概率最高的单元格被选中，并且它对应的单词被作为这个时间步的输出。其详细输入输出的流程图如Figure 7。![](https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411122219.png)------### 3.5 Positional EncodingSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the. bottoms of the encoder and decoder stacks. The positional encodings have the same dimension $$d_{model}$$ as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed. In this work, we use sine and cosine functions of different frequencies:</script><p>PE<em>{(pos,2i)}=sin(pos/10000^{2i/dmodel}) \<br>PE</em>{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})</p><script type="math/tex; mode=display">where *pos* is the position and *i* is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2*π* to 10000 *·* 2*π*. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset *k*, $$PE_{pos+k}$$ can be represented as a linear function of $$PE_{pos}$$.We also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training.解读：一个传统的，使用 RNN 构建的 Encoder-Decoder 模型对于输入句子中的单词是逐个读取的：读取完前一个单词，更新模型的状态，然后在此状态下再读取下一个单词。这种方式天然的包含了句子的位置前后关系。CNN模型可以有卷积的多次采样扩展得到位置信息。Transformer模型不包括recurrence/convolution，其对整个句子的每个单词进行同时读取，因此是无法捕捉到序列顺序信息的。假如将词序打乱，或者说将K、V按行进行打乱，那么Attention之后的结果是一样的。这就与人类的语言常识相悖，通常来看，在处理时序数据的时候，如果将一句话里面的词打乱或颠倒顺序，那么语义肯定会发生变化，但是 attention 不会处理这个情况。 因此为了能够保留句子中单词和单词之间的位置关系，需要加入时序信息。也就是需要将位置也融合进入输入的句子中，即对位置进行编码。Transformer 使用的位置编码计算公式如下：</script><p>PE<em>{(pos,2i)}=sin(pos/10000^{2i/dmodel}) \<br>PE</em>{(pos,2i+1)}=cos(pos/10000^{2i/dmodel})</p><script type="math/tex; mode=display">其中*pos*表示位置index， *i*表示dimension index。由公式来看，对于一个句子，此编码算法对于偶数位置 *2i* 和奇数位置 *2i+1* 分开进行编码。编码的结果是每个位置最终转化为$$d_{model}$$维度的向量。虽然现在知道了Transformer的位置编码方式，学习讲究知其然知其所以然，接下来再简析一下Transformer位置编码的特点和原理。将位置编码公式拆解微观分析得：![](https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411151747.png)并将位置编码向量按照热力图的形式进行展示得：![](https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411152231.png)上述横坐标代表维度*i*，纵坐标代表位置。可以看到，对于句子长度最大为50的模型来说，前60维就可以区分位置了。至于后面为什么一白一蓝，别忘了，sin,cos相间。Position Embedding本身是一个绝对位置的信息，仔细观察Transformer论文给出位置编码的公式可以发现，即只要位置小于10000，每一个位置的编码都是不同的。但在语言中，相对位置也很重要，绝对位置信息只保证了各个位置不一样，但是并不是像0,1,2这样的有明确前后关系的编码。所以需要相对位置来保证语序对应的语义。由于三角函数两角和差公式，Transformer论文给出位置编码的公式也可以很好的表示相对位置信息。</script><p>sin(\alpha+\beta) = sin(\alpha)cos(\beta) + cos(\alpha)sin(\beta)\<br>cos(\alpha+\beta) = cos(\alpha)cos(\beta) - sin(\alpha)cos(\beta)</p><p>$$<br>给定k,存在一个固定的与k相关的线性变换矩阵，从而由<em>pos</em>的位置编码线性变换而得到<em>pos+k</em>的位置编码。从而这个相对位置信息也可以被模型发现并利用。其证明如下：</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220411153722.png" alt></p><p>序列信息代表着全局的结构，因此必须将序列的token相对或者绝对position信息利用起来。这里每个token的position embedding 向量维度也是<script type="math/tex">d_{model}=512</script>，然后将原本的embedding和position embedding加起来组成最终的embedding作为encoder/decoder的输入。positional embedding是 cos 和 sin 的一个函数，在 [-1, +1] 之间抖动的。所以embedding中的权重参数都乘上了<script type="math/tex">\sqrt{d_{model}}</script>，使得乘积后的每个权重也是在差不多的 [-1, +1] 数值区间。相加完成在输入里面添加时序信息。不管怎么打乱输入序列的顺序，进入 layer 之后，输出那些值是不变的，最多是顺序发生了相应的变化。所以就直接把顺序信息直接加在数据值里。</p><p>:sailboat:positional embedding被加到embedding中。嵌入表示一个 <script type="math/tex">d_{model}</script>维空间的标记，在<script type="math/tex">d_{model}</script>维空间中有着相似含义的标记会离彼此更近。但是，嵌入并没有对在一句话中的词的相对位置进行编码。因此，当加上位置编码后，词将基于它们含义的相似度以及它们在句子中的位置，在<script type="math/tex">d_{model}</script> 维空间中离彼此更近。</p><p>在其他NLP论文中，大家也都看过position embedding，通常是一个训练的向量，但是position embedding只是extra features，有该信息会更好，但是没有性能也不会产生极大下降，因为RNN、CNN本身就能够捕捉到位置信息，但是在Transformer模型中，Position Embedding是位置信息的唯一来源，因此是该模型的核心成分，并非是辅助性质的特征。</p><p>也可以采用训练的position embedding，但是试验结果表明相差不大，因此论文选择了sin position embedding，因为</p><ol><li>这样可以直接计算embedding而不需要训练，减少了训练参数</li><li>这样允许模型将position embedding扩展到超过了training set中最长position的position，例如测试集中出现了更大的position，sin position embedding依然可以给出结果，但不存在训练到的embedding。</li></ol><hr><h2 id="参考及感谢"><a href="#参考及感谢" class="headerlink" title="参考及感谢"></a>参考及感谢</h2><p>Transformer Encoder部分：</p><p><a href="https://zhuanlan.zhihu.com/p/343286144">https://zhuanlan.zhihu.com/p/343286144</a></p><p>李沐讲解Transformer：</p><p><a href="https://zhuanlan.zhihu.com/p/452663865">https://zhuanlan.zhihu.com/p/452663865</a></p><p><a href="https://zhuanlan.zhihu.com/p/460607065">https://zhuanlan.zhihu.com/p/460607065</a></p><p>一个很粗糙的代码解读：</p><p><a href="https://baijiahao.baidu.com/s?id=1711290350168224964&amp;wfr=spider&amp;for=pc">https://baijiahao.baidu.com/s?id=1711290350168224964&amp;wfr=spider&amp;for=pc</a></p><p>Transformer理解，看这篇文章读懂了embedding为什么参数共享</p><p><a href="https://blog.csdn.net/bingmeishi/article/details/105105823">https://blog.csdn.net/bingmeishi/article/details/105105823</a></p><p>Transform训练过程：</p><p><a href="https://blog.csdn.net/longxinchen_ml/article/details/86533005">https://blog.csdn.net/longxinchen_ml/article/details/86533005</a></p><p><a href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></p><p>【从 0 开始学习 Transformer】 上篇：Transformer 搭建与理解：</p><p><a href="https://zhuanlan.zhihu.com/p/97448796">https://zhuanlan.zhihu.com/p/97448796</a></p><p>【从 0 开始学习 Transformer】 下篇：Transformer 训练与评估：</p><p><a href="https://zhuanlan.zhihu.com/p/97451231">https://zhuanlan.zhihu.com/p/97451231</a></p><p>自注意动图展示：</p><p><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a></p><p>深度学习中的注意力模型（2017版）：</p><p><a href="https://zhuanlan.zhihu.com/p/37601161">https://zhuanlan.zhihu.com/p/37601161</a></p><p>Transformer学习笔记一：Positional Encoding（位置编码）</p><p><a href="https://zhuanlan.zhihu.com/p/454482273">https://zhuanlan.zhihu.com/p/454482273</a></p><p>Transformer架构：位置编码</p><p><a href="https://blog.csdn.net/Jayson13/article/details/123135888">https://blog.csdn.net/Jayson13/article/details/123135888</a></p><p>深入理解transformer中的位置编码</p><p><a href="https://www.csdn.net/tags/OtDacg3sNDA1NjUtYmxvZwO0O0OO0O0O.html">https://www.csdn.net/tags/OtDacg3sNDA1NjUtYmxvZwO0O0OO0O0O.html</a></p><p><a href="https://blog.csdn.net/qq_43391414/article/details/121061766">https://blog.csdn.net/qq_43391414/article/details/121061766</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>经典论文Transformer的解读与厚黑---《Attention is All you need》一（Introduction和Background部分）</title>
      <link href="/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/"/>
      <url>/2022/04/08/%E7%BB%8F%E5%85%B8%E8%AE%BA%E6%96%87%E4%B8%AD%E7%9A%84%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%8E%9A%E9%BB%91---%E3%80%8AAttention%20is%20All%20you%20need%E3%80%8B%EF%BC%88Introduction%E5%92%8CBackground%E9%83%A8%E5%88%86%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="经典论文的解读与厚黑—-《Attention-is-All-you-need》一（Introduction和Background部分）"><a href="#经典论文的解读与厚黑—-《Attention-is-All-you-need》一（Introduction和Background部分）" class="headerlink" title="经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）"></a>经典论文的解读与厚黑—-《Attention is All you need》一（Introduction和Background部分）</h1><p>作者： Aqua，苑博。</p><p>审校：苑博。</p><p>翻译：Aqua。</p><p>感谢我的好兄弟苑博的倾力贡献，并祝学业顺利！</p><p>《Attention is All you need》你真的读懂了吗？不管读没读过，这篇论文都值得多读几遍。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>Recurrent neural networks, long short-term memory and gated recurrent  neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures.</p><p>解读与厚黑：</p><p>目前来看，循环神经网络（RNN），如长短期记忆和门控神经网络模型（LSTM），在语言模型和机器翻译领域一直处于主导地位。可惜的是现在的大部分研究和努力只是游离在RNN和编码器-解码器架构的边界而已，意义不大。（隐义：现在循环神经网络很火，虽然大家都在凑这个热闹，但都是些小打小闹，没啥突破性进展。是时候换换天了。为Transformer做铺垫）</p><hr><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <script type="math/tex">h_t</script> , as a function of the previous hidden state <script type="math/tex">h_{t-1}</script>​​​ and the input for position <em>t</em>. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks [18] and conditional computation [26], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains.</p><p>解读与厚黑：</p><p>简单说说RNN存在的问题， t时刻的计算<script type="math/tex">h_t</script>依赖于 t-1 时刻的计算结果<script type="math/tex">h_{t-1}</script>，这种由前往后顺序计算的形式，会存在长距离的依赖问题，这样不仅限制了模型的并行能力。还使得当前词语获取到的上下文信息受限。即使现在的很多研究想通过factorization tricks 和conditional computation来提升模型的计算效率和性能表现，但是依然没有改变RNN天生顺序约束的缺陷。（隐义：RNN的这种顺序的结构设计就有问题，你们这些跟随者即使做再多RNN局部优化的尝试，也多是隔靴挠痒，意义不大。只有从根本上改变，才能实现真正的突破，比如注意力机制）</p><hr><p>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences. In all but a few cases [22], however, such attention mechanisms are used in conjunction with a recurrent network.</p><p>解读与厚黑：</p><p>众所周知，在翻译过程中，不同的英文对中文的依赖程度不同。还有文本在表征语义时，当前词需要获取的信息可能来自距离很远的另一个词。 注意力机制（Attention mechanisms）通过将序列中任意两个位置之间的距离缩小为一个常量 ，从而解决长距离依赖的问题。这种对依赖关系进行建模的方式，不仅可以很好的适应了我们人脑的认知，还能够实现并行。在大多数情况下，注意机制与循环网络一起使用往往会产生很好的效果。（隐义：你看注意力机制就不一样了吧，把你最根本的问题解决了，并且我还发现Attention搭配上RNN，效果倍棒。方向给你们了，自己看着办！）</p><hr><p>In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.</p><p>解读与厚黑：</p><p>Transformer的核心就是使用Attention来绘制输入和输出之间的全局依赖关系，除此之外，还可以使用并行训练的方式大大缩短训练时间。（隐义：咱们这个Transformer内部用的可是Attention，训练快效果好，洋气又时髦，了解一下啊大哥）</p><hr><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><p>The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet  and ConvS2S, all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions.  In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [11]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2.</p><p>解读与厚黑：</p><p>RNN的最大问题在于顺序计算，虽然通过卷积神经网络的方式可以减少顺序计算，实现并行计算。但是模型在学习任意两个位置之间的依赖关系时，所需的操作数随着距离的增长而增长。这就使得模型学习遥远位置之间的依赖关系要更加困难。而在Transformer中，学习任意两个位置之间的依赖关系所需的操作数被减少为恒定次数。所以Transformer要相比于卷积的形式就来的更加有效率。（隐义：虽然你卷积可以并行，但是你有长距离难计算的问题啊，我的Transformer就不同了，不仅具有你并行的好处，还能轻松计算任意位置间的依赖关系。卷积你不太行啊！）</p><hr><p>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations.</p><p>解读与厚黑：</p><p>自注意机制作为注意机制中的一种，将单个序列的不同位置联系起来，以计算序列的表示。同时自注意机制已成功地应用于各种任务中，包括阅读理解、抽象摘要和文本隐含等。（隐义：自注意力机制对各种NLP任务的适用性都挺好，东西不错，Transformer也用用。）</p><hr><p>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks</p><p>直接厚黑：</p><p>End-to-end memory networks也是个好东西，借鉴一下。</p><hr><p>To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models.</p><p>解读与厚黑：</p><p>Transformer 是第一个完全依赖于Self-attention来计算其输入和输出表示的transduction model，而不使用序列对齐的RNN或卷积神经网络。（隐义：Transformer结合了Self-attention和End-to-end memory networks的好处，可以说是开创性尝试，下面正式开吹。）</p><hr><h2 id="参考及感谢"><a href="#参考及感谢" class="headerlink" title="参考及感谢"></a>参考及感谢</h2><p>LSTM 已死，事实真是这样吗？</p><p><a href="https://view.inews.qq.com/a/20220325A03LNX00">https://view.inews.qq.com/a/20220325A03LNX00</a></p><p>Attention is all you need 详解Transformer</p><p><a href="https://www.cnblogs.com/zhanghaiyan/p/11079504.html">https://www.cnblogs.com/zhanghaiyan/p/11079504.html</a></p><p>Attention is all you need博客</p><p><a href="https://blog.csdn.net/sinat_33741547/article/details/85884052">https://blog.csdn.net/sinat_33741547/article/details/85884052</a></p><p>论文解读:Attention is All you need</p><p><a href="https://zhuanlan.zhihu.com/p/46990010">https://zhuanlan.zhihu.com/p/46990010</a></p><p>长依赖问题：</p><p><a href="https://zhuanlan.zhihu.com/p/69704935">https://zhuanlan.zhihu.com/p/69704935</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文解读 </category>
          
          <category> 深度学习 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文解读 </tag>
            
            <tag> NLP常用算法 </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>面试算法100问---两数相加</title>
      <link href="/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/"/>
      <url>/2022/04/06/%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="面试算法100问—两数相加"><a href="#面试算法100问—两数相加" class="headerlink" title="面试算法100问—两数相加"></a>面试算法100问—两数相加</h1><p>面试官：Samira</p><p>应聘者：Aqua</p><p>Samira：”接下来我们做道题吧！“</p><p>Aqua（紧张）：”可以的。“</p><p>Samira在视频面试的做题面板上开始不紧不慢的敲击键盘。Aqua本以为Samira会敲出一道完整且表述清晰的题目，可惜他多想了，Samira只是简单给了一个数组nums&#x3D;[2,7,11,15]和target&#x3D;9，然后开始口述。</p><p>Samira：”这个数组nums和target可以看到吧，你现在要在nums中找到和为target的两个整数，然后返回这两个整数的索引。“</p><p>Aqua直接满脸问号，第一次面本来就紧张，还是口述题目，口述也就算了，关键面试官还带口音。直接当场麻掉。</p><p>Samira（很轻松）：”你不用那么紧张，如果有思路可以先聊聊看。“</p><p>Aqua思考了一会，好像回忆起来了，这个听着有点像那个两数相加。</p><p>Aqua（保持假笑）：“请问还有其他要求吗？比如每个nums只有一个答案，同时nums中同一个元素在答案中不能重复出现。”</p><p>Samira：“很好，要求如你所说，并且你可以按任意顺序返回答案。”</p><p>Aqua一边在心里抱怨，为什么不讲清楚，面试官都是玩猜的吗？一边在心里亲切的问候着Samira的家人。然后又继续想了一分钟</p><p>下边附加上LeetCode原题描述：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target  的那 两个 整数，并返回它们的数组下标。</span><br><span class="line">你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。</span><br><span class="line">你可以按任意顺序返回答案。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">1</span>：</span><br><span class="line">输入：nums = [<span class="number">2</span>,<span class="number">7</span>,<span class="number">11</span>,<span class="number">15</span>], target = <span class="number">9</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">解释：因为 nums[<span class="number">0</span>] + nums[<span class="number">1</span>] == <span class="number">9</span> ，返回 [<span class="number">0</span>, <span class="number">1</span>] 。</span><br><span class="line"></span><br><span class="line">示例 <span class="number">2</span>：</span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">2</span>,<span class="number">4</span>], target = <span class="number">6</span></span><br><span class="line">输出：[<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">示例 <span class="number">3</span>：</span><br><span class="line">输入：nums = [<span class="number">3</span>,<span class="number">3</span>], target = <span class="number">6</span></span><br><span class="line">输出：[<span class="number">0</span>,<span class="number">1</span>]</span><br></pre></td></tr></table></figure><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406203823.png"></p><p>Aqua：“我需要从nums中找到，唯一的一对索引答案[pos1,pos2]，假设我是按照顺序输出的，即pos1在pos2之前，如果如果能够找到pos2，那么只需要在pos2位置之前的元素中寻找pos1即可。这样我便可以遍历nums，假设当前遍历到nums[i]的索引 i （0&lt;&#x3D;i&lt;n）是满足题意的pos2，那么只需看在索引 i 之前是否存在target-nums[i]，若存在，这元素target-nums[i]的索引 j 即为pos1，索引 i 即为pos2。现在的问题是怎么找target-nums[i]，难道从0到 i 再遍历一遍nums，这样去寻找target-nums[i]这样的话时间复杂度就是O(n)。而寻找pos2的过程的时间复杂度为O(n)，这样总的时间复杂度就是O(n*n)。”</p><p>Simara：“可以再降低一下时间复杂度吗？”</p><p><img src="https://fanhqbucket.oss-cn-beijing.aliyuncs.com/img/20220406212608.png"></p><p>Aqua：“那就牺牲空间来减少时间吧，如果我能够在每次遍历到nums[i]的时候都能够把nums[i]之前的元素存储在HashMap中，把nums[j]当做key，把索引j作为value，0&lt;&#x3D;j&lt;i。这样在HashMap中寻找target-nums[i]也就是寻找pos1的时间复杂度为O(1)，寻找pos2的过程中需要遍历nums时间复杂度为O(n)，所以总的时间复杂度为O(n)。同时，因为这个过程用到了HashMap，所以空间复杂度为O(n)。“</p><p>Simara：“你是想把所有的nums中元素和其对应的索引一次性全部放到HashMap中，还是在遍历过程中进行动态扩充HashMap？”</p><p>Aqua：“进行动态扩充吧，如果我在遍历nums的过程其实就是寻找pos2的过程，如果遍历到的nums[i]的索引 i 不是答案中pos2，也就是在nums[i]之前不存在target-nums[i]，那么说明pos2还在当前 i 的后面，也就是说这个nums[i] 和索引 i 是需要加入到HashMap中的，那么直接加入HashMap。”</p><p>Simara：“那按照你的思路实现一下代码吧！”</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Solution</span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span>[] twoSum(<span class="type">int</span>[] nums, <span class="type">int</span> target) &#123;</span><br><span class="line">    <span class="keyword">if</span>(nums == <span class="literal">null</span> || nums.length == <span class="number">0</span>)&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="title class_">int</span>[]&#123;<span class="number">0</span>,<span class="number">0</span>&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">int</span>[] pos = <span class="keyword">new</span> <span class="title class_">int</span>[<span class="number">2</span>];</span><br><span class="line">    HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;Integer,Integer&gt;();</span><br><span class="line">    <span class="type">int</span> <span class="variable">len</span> <span class="operator">=</span> nums.length;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; len; i++)&#123;</span><br><span class="line">    <span class="keyword">if</span>(map.containsKey(target-nums[i]))&#123;</span><br><span class="line">    pos[<span class="number">0</span>] = map.get(target-nums[i]);</span><br><span class="line">    pos[<span class="number">1</span>] = i;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    map.put(nums[i], i);</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">return</span> pos;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 面试算法100问 </category>
          
          <category> HashMap </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试算法题 </tag>
            
            <tag> 简单算法题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>启程算法之旅</title>
      <link href="/2022/03/26/%E5%90%AF%E7%A8%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E6%97%85/"/>
      <url>/2022/03/26/%E5%90%AF%E7%A8%8B%E7%AE%97%E6%B3%95%E4%B9%8B%E6%97%85/</url>
      
        <content type="html"><![CDATA[<h1 id="博客说明"><a href="#博客说明" class="headerlink" title="博客说明"></a>博客说明</h1><p>自己一直有记笔记的习惯，感觉这样可以很好减少重复学习的时间。虽然现在CSDN和知乎都可以通过博客的方式来记录学习内容，但是总感觉他们那个书写界面不太适合自己。所以平常都是通过Typora记录在本地，随着记录的内容不断增多，感觉总是在本地和手机之间传来传去有点不太方便。所以就想做个个人博客吧，这样既能继续保留Typora写文档的习惯，还能把学的东西系统化的展示出来，同时也能看的方便。这么一琢磨，确实是时候做一做了。</p><p>经过两天的学习，终于把这个博客搞完了。除了界面的美化不能和专业前端相比之外，现有的功能已经基本满足个人的使用需求。根据目前的情况，以后也会不定期的更新博客，后续内容主要分两方面：第一方面主要是一些常见的机器学习算法的应用和理论（LR，SVM，随机森林，决策树，GBDT，XGboost，LightGBM，贝叶斯网络，HMM，CRF和聚类等）。对于深度学习这一块，因为自己主要搞NLP方向，所以会集中讲一些深度学习框架应用（CNN，RNN，Word2Vector，LSTM，Transformer，BERT，RoBert，XLBert和XLNet等）。最后是希望自己可以保证精产还能多产。当然最重要的是希望通过写博客这样一种方式，可以让自己的学习过程能更扎实一些。也让自己对技术的掌握更熟练一些。毕竟技术的尽头还是技术。</p><h2 id="Hexo个人常用命令"><a href="#Hexo个人常用命令" class="headerlink" title="Hexo个人常用命令"></a>Hexo个人常用命令</h2><p>为了减少重复搜索，还是记下来比较好</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo s <span class="comment">#启动本地静态页面</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo g <span class="comment">#生成静态文件</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo d <span class="comment">#部署到远程站点</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
          <category> Hexo使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
            <tag> Hexo框架 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2022/03/25/hello-world/"/>
      <url>/2022/03/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      <categories>
          
          <category> 技能学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客搭建 </tag>
            
            <tag> 个人建设 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
